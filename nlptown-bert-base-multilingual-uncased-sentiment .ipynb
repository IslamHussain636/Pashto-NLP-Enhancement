{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7602802,"sourceType":"datasetVersion","datasetId":4426050},{"sourceId":7639691,"sourceType":"datasetVersion","datasetId":4452458},{"sourceId":7642463,"sourceType":"datasetVersion","datasetId":4454338}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Loading the important libraris","metadata":{"id":"8AqBTNmLGhmN"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nfrom keras.layers import Embedding, Dropout, Dense, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPool1D\nfrom keras.models import Sequential\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport itertools\nimport json","metadata":{"id":"l-YbOJdXGm-a","execution":{"iopub.status.busy":"2024-02-18T21:36:14.485075Z","iopub.execute_input":"2024-02-18T21:36:14.485681Z","iopub.status.idle":"2024-02-18T21:36:28.591427Z","shell.execute_reply.started":"2024-02-18T21:36:14.485644Z","shell.execute_reply":"2024-02-18T21:36:28.590609Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-18 21:36:17.151567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-18 21:36:17.151667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-18 21:36:17.294225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"G41zdyKYGvC3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LjHEIlyLIS_b","outputId":"c932b2e4-7d77-44a8-83ba-523bcfd8a642","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9G25d6saKsx2","outputId":"5c5a4799-8bdc-493c-ebef-8f65d6b2b215","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgLFd_vHqkk9","outputId":"77543024-b101-4a14-d50b-911e90d60266","execution":{"iopub.status.busy":"2024-02-18T21:43:40.755005Z","iopub.execute_input":"2024-02-18T21:43:40.755674Z","iopub.status.idle":"2024-02-18T21:44:03.996584Z","shell.execute_reply.started":"2024-02-18T21:43:40.755645Z","shell.execute_reply":"2024-02-18T21:44:03.995480Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nCollecting transformers\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\nSuccessfully installed transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing important libraries\nimport torch\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"id":"6MKGelDSRbq6","execution":{"iopub.status.busy":"2024-02-18T22:00:26.485273Z","iopub.execute_input":"2024-02-18T22:00:26.486162Z","iopub.status.idle":"2024-02-18T22:00:26.497641Z","shell.execute_reply.started":"2024-02-18T22:00:26.486128Z","shell.execute_reply":"2024-02-18T22:00:26.496860Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# seed everythig\n\nSEED_VAL = 42\ntorch.manual_seed(SEED_VAL)\ntorch.cuda.manual_seed_all(SEED_VAL)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"XXVYHxqCqkk-","execution":{"iopub.status.busy":"2024-02-18T22:00:29.345481Z","iopub.execute_input":"2024-02-18T22:00:29.346192Z","iopub.status.idle":"2024-02-18T22:00:29.354313Z","shell.execute_reply.started":"2024-02-18T22:00:29.346160Z","shell.execute_reply":"2024-02-18T22:00:29.353165Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\n\nMAX_LENGTH = 100\nLR = 2e-5\nBATCH_SIZE = 16\nEPOCHS = 3","metadata":{"id":"LZfAf6RKqkk-","execution":{"iopub.status.busy":"2024-02-18T22:00:30.835388Z","iopub.execute_input":"2024-02-18T22:00:30.836072Z","iopub.status.idle":"2024-02-18T22:00:30.840189Z","shell.execute_reply.started":"2024-02-18T22:00:30.836037Z","shell.execute_reply":"2024-02-18T22:00:30.839183Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# model and tokenizer\n\nMODEL = 'nlptown/bert-base-multilingual-uncased-sentiment'","metadata":{"id":"59lnI9X5qkk-","execution":{"iopub.status.busy":"2024-02-18T22:02:42.159285Z","iopub.execute_input":"2024-02-18T22:02:42.160038Z","iopub.status.idle":"2024-02-18T22:02:42.163988Z","shell.execute_reply.started":"2024-02-18T22:02:42.160005Z","shell.execute_reply":"2024-02-18T22:02:42.163008Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# loading the data\ndf = pd.read_csv('/kaggle/input/balanced-data/balanced_data.csv')\ntrain, test = train_test_split(df, test_size=0.2, stratify=df.label, random_state=SEED_VAL)\ntest, val = train_test_split(test, test_size=0.5, stratify=test.label, random_state=SEED_VAL)\n\ntrain_text = list(train.text)\ntrain_labels = list(train.label)\n\nval_text = list(val.text)\nval_labels = list(val.label)\n\ntest_text = list(test.text)\ntest_labels = list(test.label)\n\nprint('Total: ', len(df))\nprint('Train: ', len(train))\nprint('Val: ', len(val))\nprint('Test: ', len(test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oH272kgNqkk-","outputId":"246d449f-1b8f-433d-c2d8-56cd41a7cf95","execution":{"iopub.status.busy":"2024-02-18T22:02:43.615385Z","iopub.execute_input":"2024-02-18T22:02:43.616171Z","iopub.status.idle":"2024-02-18T22:02:43.815463Z","shell.execute_reply.started":"2024-02-18T22:02:43.616140Z","shell.execute_reply":"2024-02-18T22:02:43.814508Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Total:  44000\nTrain:  35200\nVal:  4400\nTest:  4400\n","output_type":"stream"}]},{"cell_type":"code","source":"# df['text'][1421]\ndf","metadata":{"execution":{"iopub.status.busy":"2024-02-18T22:02:44.585394Z","iopub.execute_input":"2024-02-18T22:02:44.585777Z","iopub.status.idle":"2024-02-18T22:02:44.597290Z","shell.execute_reply.started":"2024-02-18T22:02:44.585747Z","shell.execute_reply":"2024-02-18T22:02:44.596191Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0      د جنګ د مفهوم او معنا څخه ناخبره معصوم ماشومان...      0\n1      تردې وروسته يې خپله دوکتورا د وګړپوهنې په څانګ...      0\n2      د افغانستان د کرکټ لوبغاړي اماراتو ته رسېدلي د...      0\n3                     د بابا تازه انځور ته زړونه ورکړئ ❤      0\n4                                       البيت الابراهيمي      0\n...                                                  ...    ...\n43995  ستا مور له عزته سره وغيم فراري دي مور غولي ملي...      1\n43996  ستا مفکوره د پنجاسپیتوب مفکوره ده وړه نجلکۍ یې...      1\n43997  ددناموس وغیم خوچی نیسی یی مردار ه وایی ولی هسی...      1\n43998  ډیر عجب ښه کار وشو داسی خبیث خلک همداسی سزا ور...      1\n43999  د الله لعنت د پرتا وې خاینه فاسده تا د افغانست...      1\n\n[44000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>د جنګ د مفهوم او معنا څخه ناخبره معصوم ماشومان...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>تردې وروسته يې خپله دوکتورا د وګړپوهنې په څانګ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>د افغانستان د کرکټ لوبغاړي اماراتو ته رسېدلي د...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>د بابا تازه انځور ته زړونه ورکړئ ❤</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>البيت الابراهيمي</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43995</th>\n      <td>ستا مور له عزته سره وغيم فراري دي مور غولي ملي...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43996</th>\n      <td>ستا مفکوره د پنجاسپیتوب مفکوره ده وړه نجلکۍ یې...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43997</th>\n      <td>ددناموس وغیم خوچی نیسی یی مردار ه وایی ولی هسی...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43998</th>\n      <td>ډیر عجب ښه کار وشو داسی خبیث خلک همداسی سزا ور...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43999</th>\n      <td>د الله لعنت د پرتا وې خاینه فاسده تا د افغانست...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>44000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenization\n# from transformers import DistilBertTokenizerFast , TFDistilBertForSequenceClassification\n\ntokenizer =BertTokenizer.from_pretrained(MODEL)\ntrain_encodings = tokenizer(train_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\nval_encodings = tokenizer(val_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\ntest_encodings = tokenizer(test_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_1_KBJ1qkk_","outputId":"28e483e6-474b-4ce6-f491-ceb5fadaca9e","execution":{"iopub.status.busy":"2024-02-18T22:02:45.413548Z","iopub.execute_input":"2024-02-18T22:02:45.414405Z","iopub.status.idle":"2024-02-18T22:03:18.558833Z","shell.execute_reply.started":"2024-02-18T22:02:45.414373Z","shell.execute_reply":"2024-02-18T22:03:18.557723Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa0d0d5f6d54509be8a7fb7a962d012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5348c3999d641c8be7419d4f575c87b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c205aa1fe344539964ee807a469fb6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093eb4d73d074db3806aaabddd00c9bf"}},"metadata":{}}]},{"cell_type":"code","source":"# Torch Dataset\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = MyDataset(train_encodings, train_labels)\nval_dataset = MyDataset(val_encodings, val_labels)\ntest_dataset = MyDataset(test_encodings, test_labels)","metadata":{"id":"tStNX5Coqkk_","execution":{"iopub.status.busy":"2024-02-18T22:03:41.233596Z","iopub.execute_input":"2024-02-18T22:03:41.234253Z","iopub.status.idle":"2024-02-18T22:03:41.240872Z","shell.execute_reply.started":"2024-02-18T22:03:41.234222Z","shell.execute_reply":"2024-02-18T22:03:41.239872Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] -U\n!pip install accelerate -U\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0NprsErswHc","outputId":"686d6d4d-07c0-444e-ff99-7f4827bf737f","execution":{"iopub.status.busy":"2024-02-18T22:03:42.975134Z","iopub.execute_input":"2024-02-18T22:03:42.975764Z","iopub.status.idle":"2024-02-18T22:04:08.385775Z","shell.execute_reply.started":"2024-02-18T22:03:42.975732Z","shell.execute_reply":"2024-02-18T22:04:08.384579Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.37.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.11 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.26.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\nCollecting accelerate\n  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.26.1\n    Uninstalling accelerate-0.26.1:\n      Successfully uninstalled accelerate-0.26.1\nSuccessfully installed accelerate-0.27.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training Arguments\n\n\n!pip install transformers[torch]\n\nfrom transformers import TrainingArguments, Trainer\n\n# Rest of your code\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    logging_steps=50,\n    save_strategy='no',\n    evaluation_strategy='steps',\n    logging_strategy='steps',\n    report_to='none',\n)\n\n# Continue with your Trainer setup and training\n","metadata":{"id":"y1-C6yBRqkk_","execution":{"iopub.status.busy":"2024-02-18T22:04:08.387828Z","iopub.execute_input":"2024-02-18T22:04:08.388136Z","iopub.status.idle":"2024-02-18T22:04:20.671043Z","shell.execute_reply.started":"2024-02-18T22:04:08.388106Z","shell.execute_reply":"2024-02-18T22:04:20.669766Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.37.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.11 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model and Trainer\n\nmodel = BertForSequenceClassification.from_pretrained(MODEL)\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)","metadata":{"id":"tI-Qt-8rqklA","execution":{"iopub.status.busy":"2024-02-18T22:04:20.672446Z","iopub.execute_input":"2024-02-18T22:04:20.672753Z","iopub.status.idle":"2024-02-18T22:04:24.515282Z","shell.execute_reply.started":"2024-02-18T22:04:20.672720Z","shell.execute_reply":"2024-02-18T22:04:24.514487Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66b206c66d6468289168709396d76fc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"# Start Fine-tuning\n\ntrainer.train()","metadata":{"id":"15eQckVUqklA","execution":{"iopub.status.busy":"2024-02-18T22:04:24.516908Z","iopub.execute_input":"2024-02-18T22:04:24.517201Z","iopub.status.idle":"2024-02-18T22:54:14.317853Z","shell.execute_reply.started":"2024-02-18T22:04:24.517176Z","shell.execute_reply":"2024-02-18T22:54:14.316872Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6600' max='6600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6600/6600 49:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.630300</td>\n      <td>0.604264</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.486600</td>\n      <td>0.602333</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.478000</td>\n      <td>0.509333</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.457400</td>\n      <td>0.343803</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.378000</td>\n      <td>0.335204</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.394700</td>\n      <td>0.361867</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.373700</td>\n      <td>0.302759</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.293800</td>\n      <td>0.285598</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.317900</td>\n      <td>0.291522</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.320500</td>\n      <td>0.261230</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.283200</td>\n      <td>0.313081</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.318900</td>\n      <td>0.323888</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.297100</td>\n      <td>0.256214</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.249300</td>\n      <td>0.255843</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.267300</td>\n      <td>0.293614</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.275300</td>\n      <td>0.270580</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.260300</td>\n      <td>0.249595</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.296100</td>\n      <td>0.232726</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.295800</td>\n      <td>0.247134</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.275500</td>\n      <td>0.241571</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.269800</td>\n      <td>0.242929</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.259400</td>\n      <td>0.303684</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.298700</td>\n      <td>0.274059</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.281300</td>\n      <td>0.230374</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.270300</td>\n      <td>0.218950</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.276100</td>\n      <td>0.240387</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.291000</td>\n      <td>0.254506</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.250300</td>\n      <td>0.227585</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.218700</td>\n      <td>0.248925</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.269700</td>\n      <td>0.213607</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.253500</td>\n      <td>0.234276</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.297800</td>\n      <td>0.202423</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.258300</td>\n      <td>0.202103</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.256100</td>\n      <td>0.204462</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.256300</td>\n      <td>0.204355</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.249500</td>\n      <td>0.207933</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.245500</td>\n      <td>0.253115</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.225000</td>\n      <td>0.223331</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.201800</td>\n      <td>0.210157</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.256400</td>\n      <td>0.194955</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.246800</td>\n      <td>0.214966</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.268100</td>\n      <td>0.268651</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.209100</td>\n      <td>0.217168</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.261600</td>\n      <td>0.217850</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.196600</td>\n      <td>0.241820</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.227600</td>\n      <td>0.201252</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.203700</td>\n      <td>0.200136</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.155600</td>\n      <td>0.269675</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.203800</td>\n      <td>0.218390</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.177500</td>\n      <td>0.210806</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.181000</td>\n      <td>0.224687</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.208300</td>\n      <td>0.249075</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.213900</td>\n      <td>0.212133</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.167200</td>\n      <td>0.233673</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.186400</td>\n      <td>0.236853</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.220200</td>\n      <td>0.203000</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.202900</td>\n      <td>0.187382</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.205700</td>\n      <td>0.223426</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.199900</td>\n      <td>0.244678</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.199800</td>\n      <td>0.222668</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.196400</td>\n      <td>0.195751</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.172500</td>\n      <td>0.208717</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.211100</td>\n      <td>0.180509</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.145000</td>\n      <td>0.210215</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.209000</td>\n      <td>0.181574</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.163500</td>\n      <td>0.196109</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.148600</td>\n      <td>0.256029</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.186600</td>\n      <td>0.205695</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.184300</td>\n      <td>0.207077</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.181800</td>\n      <td>0.218866</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.156800</td>\n      <td>0.210803</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.250900</td>\n      <td>0.164156</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.137000</td>\n      <td>0.233078</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.155500</td>\n      <td>0.202672</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.157900</td>\n      <td>0.210912</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.199000</td>\n      <td>0.189619</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.160200</td>\n      <td>0.194001</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.172600</td>\n      <td>0.198783</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.178800</td>\n      <td>0.212935</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.134900</td>\n      <td>0.216010</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.187500</td>\n      <td>0.206617</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.143300</td>\n      <td>0.190591</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.151400</td>\n      <td>0.209538</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.161400</td>\n      <td>0.195776</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.208900</td>\n      <td>0.189176</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.184400</td>\n      <td>0.165540</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.148500</td>\n      <td>0.211148</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.227200</td>\n      <td>0.204615</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.123100</td>\n      <td>0.193572</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.114400</td>\n      <td>0.221578</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.163700</td>\n      <td>0.213642</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.137300</td>\n      <td>0.209555</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.112500</td>\n      <td>0.213299</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.127700</td>\n      <td>0.227104</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.136300</td>\n      <td>0.218547</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.132900</td>\n      <td>0.216652</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.090800</td>\n      <td>0.251152</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.111400</td>\n      <td>0.262619</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.128300</td>\n      <td>0.237021</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.159100</td>\n      <td>0.213879</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.101600</td>\n      <td>0.229636</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.123900</td>\n      <td>0.236435</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.126500</td>\n      <td>0.249289</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.143100</td>\n      <td>0.238087</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.103900</td>\n      <td>0.240869</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.124500</td>\n      <td>0.234245</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.145800</td>\n      <td>0.239542</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.128500</td>\n      <td>0.222497</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.113900</td>\n      <td>0.232809</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.093500</td>\n      <td>0.235234</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.079700</td>\n      <td>0.246573</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.135800</td>\n      <td>0.244059</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.119000</td>\n      <td>0.230474</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.153500</td>\n      <td>0.214034</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.116900</td>\n      <td>0.229597</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.075600</td>\n      <td>0.227453</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.125000</td>\n      <td>0.228117</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.156100</td>\n      <td>0.218911</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.131900</td>\n      <td>0.220720</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.091100</td>\n      <td>0.223046</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.128500</td>\n      <td>0.235152</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.100400</td>\n      <td>0.230321</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.179900</td>\n      <td>0.224363</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.096600</td>\n      <td>0.225242</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.117000</td>\n      <td>0.220839</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.137300</td>\n      <td>0.222345</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.099800</td>\n      <td>0.220989</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.128800</td>\n      <td>0.221589</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.142000</td>\n      <td>0.219912</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.140500</td>\n      <td>0.219351</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.156000</td>\n      <td>0.218449</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.110900</td>\n      <td>0.218541</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6600, training_loss=0.20189584750117678, metrics={'train_runtime': 2989.2946, 'train_samples_per_second': 35.326, 'train_steps_per_second': 2.208, 'total_flos': 5426811688320000.0, 'train_loss': 0.20189584750117678, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# trainer.evaluate()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"aoWd0sRpwsY_","outputId":"8a023bf5-4411-48c9-d0a0-b3501cbd5e8d","execution":{"iopub.status.busy":"2024-02-18T19:58:18.718980Z","iopub.execute_input":"2024-02-18T19:58:18.719238Z","iopub.status.idle":"2024-02-18T19:58:18.723049Z","shell.execute_reply.started":"2024-02-18T19:58:18.719214Z","shell.execute_reply":"2024-02-18T19:58:18.722221Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# print(logs.columns)\n#","metadata":{"id":"3EJuSreqwQr9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss curves\nlogs = pd.DataFrame(trainer.state.log_history)\nlogs.to_csv('history.txt', sep='\\t', index=None)\n","metadata":{"id":"f64B40YPqklB","execution":{"iopub.status.busy":"2024-02-18T21:17:07.174304Z","iopub.execute_input":"2024-02-18T21:17:07.175061Z","iopub.status.idle":"2024-02-18T21:17:07.189063Z","shell.execute_reply.started":"2024-02-18T21:17:07.175031Z","shell.execute_reply":"2024-02-18T21:17:07.188115Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# # Assuming logs is your DataFrame containing the training logs\n# logs = pd.DataFrame(trainer.state.log_history)\n\n# # Filter out any steps where the loss is not greater than  0\n# loss = logs.loc[logs['train_loss'] >  0]\n\n# # Plot the training loss\n# plt.figure(figsize=(2.8,  2))\n# plt.plot(loss.step, loss['train_loss'], label='Train Loss', lw=1)\n# plt.legend(['Train Loss'], loc='upper right')\n# plt.xlabel('Steps', fontsize=11)\n# plt.ylabel('Loss', fontsize=11)\n# plt.title('Pashto-BERT')\n# plt.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"3wglRrZtxsMC","outputId":"a0c61ba7-1936-41a2-c2b5-187ea1d0c8c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing and prediction\npreds_raw, test_labels , _ = trainer.predict(test_dataset)\npreds = np.argmax(preds_raw, axis=-1)\nprint(classification_report(test_labels, preds, digits=4))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"ybG-HZztqklB","outputId":"dd66df7b-12c8-430c-bcad-a903952c3765","execution":{"iopub.status.busy":"2024-02-18T22:54:58.335600Z","iopub.execute_input":"2024-02-18T22:54:58.336024Z","iopub.status.idle":"2024-02-18T22:55:12.429484Z","shell.execute_reply.started":"2024-02-18T22:54:58.335991Z","shell.execute_reply":"2024-02-18T22:55:12.428543Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9401    0.9345    0.9373      2200\n           1     0.9349    0.9405    0.9377      2200\n\n    accuracy                         0.9375      4400\n   macro avg     0.9375    0.9375    0.9375      4400\nweighted avg     0.9375    0.9375    0.9375      4400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# confusion matrix\ncm = confusion_matrix(test_labels, preds)\nplt.figure(figsize=(2,2))\nplt.imshow(cm, interpolation='nearest', cmap='Blues')\nplt.title('bert-base-multilingual-uncased-sentiment')\ntick_marks = np.arange(2)\nplt.xticks([0,1])\nplt.yticks([0,1])\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"xG6T-L7fqklC","outputId":"26d3ff1f-cd04-40bc-f6e8-efbc9ca4bde9","execution":{"iopub.status.busy":"2024-02-18T22:57:44.146501Z","iopub.execute_input":"2024-02-18T22:57:44.147201Z","iopub.status.idle":"2024-02-18T22:57:44.290385Z","shell.execute_reply.started":"2024-02-18T22:57:44.147167Z","shell.execute_reply":"2024-02-18T22:57:44.289635Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXEAAADcCAYAAACGcpEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmHklEQVR4nO3deVxU9f4/8NewDTuyIwqCaBmkVGOilYKKAm65UldNxFRKsdJIxXsNLc3ccUEtLTHt3sz1ql/LxBX3EPXaogmBkqRsxqaAMp/fH/w4eZxBEYXh1Ov5ePCo85nPOZ/3mTnzmsPnHEaVEEKAiIgUycjQBRARUd0xxImIFIwhTkSkYAxxIiIFY4gTESkYQ5yISMEY4kRECsYQJyJSMIY4EZGCPXSIz5gxAyqVCnl5efVRzyNpzLU1BpmZmVCpVEhMTKxVf5VKhRkzZkjLiYmJUKlUyMzMlNqCgoIQFBT0WOtsCNXHCtWOl5cXRo4caegyHpq+Y/avplGfia9YsaLWgUN1s3v3bllQEynRRx99hO3btxu6jDp7lKxjiP/N7d69GzNnztT72K1bt/Cvf/3rvut/9913+O677+qjNKJaqynEX3vtNdy6dQstWrRo+KIewqNkncnjLeXxuHnzJiwtLQ1dxt+eubn5A/uYmZk1QCVEdWNsbAxjY2NDl1Gv6nwmnpeXh/DwcNja2sLR0RFvv/02ysrKZH02bNgAjUYDCwsLODg44NVXX0VWVpasT1BQEJ5++mmcPn0aXbp0gaWlJaZNmwYvLy/8+OOPOHToEFQqFVQqVa3nXmtT29q1a9GtWze4uLhArVbD19cXK1eu1NlWSkoKQkJC4OTkBAsLC3h7e2PUqFGyPlqtFvHx8fDz84O5uTlcXV0RFRWFGzdu1KpelUqF6OhobNq0Cb6+vrCwsECnTp1w/vx5AMAnn3yCVq1awdzcHEFBQTrzezXNVz5ovnrkyJFISEiQaqj+ubuuB0213DvGwYMHoVKp8PXXX2P27Nlo3rw5zM3N0b17d6Slpemsn5CQgJYtW8LCwgIdOnRAcnKyzjZrmtesHuvgwYNSW3JyMoYMGQJPT0+o1Wp4eHhg4sSJuHXr1n33oyb6xgD0X18YOXIkrK2tcfXqVfTv3x/W1tZwdnZGTEwMKisrZetrtVosWbIEbdu2hbm5OZydnREaGoqUlBSpjyGOUSEEZs2ahebNm8PS0hJdu3bFjz/+WOvnq7i4GO+88w68vLygVqvh4uKCHj16IDU1Vdbv5MmTCA0NhZ2dHSwtLREYGIijR4/K+lRft0hLS8PIkSPRpEkT2NnZITIyEjdv3pT6qVQqlJaWYt26ddIxXP1+0HfseHl5oU+fPjh48CDat28PCwsLtG3bVnqNt27dKr0uGo0GZ86c0dnPCxcuYPDgwXBwcIC5uTnat2+PHTt2yPpUj3306FFMmjQJzs7OsLKywoABA5Cbmyurp65ZBzzCmXh4eDi8vLwwZ84cnDhxAkuXLsWNGzfwxRdfAABmz56N6dOnIzw8HKNHj0Zubi6WLVuGLl264MyZM2jSpIm0rfz8fISFheHVV1/F8OHD4erqiqCgIEyYMAHW1tb45z//CQBwdXV9LLUBwMqVK+Hn54d+/frBxMQEO3fuxLhx46DVajF+/HgAQE5ODnr27AlnZ2dMnToVTZo0QWZmJrZu3SobLyoqComJiYiMjMRbb72FjIwMLF++HGfOnMHRo0dhamr6wJqTk5OxY8cOaew5c+agT58+mDx5MlasWIFx48bhxo0bmDdvHkaNGoX9+/fX6rm4n6ioKGRnZ2Pv3r1Yv379I2/vbh9//DGMjIwQExODwsJCzJs3D8OGDcPJkyelPitXrkR0dDQ6d+6MiRMnIjMzE/3794e9vT2aN29ep3E3bdqEmzdv4s0334SjoyNOnTqFZcuW4bfffsOmTZse1+7VqLKyEiEhIQgICMCCBQuQlJSEhQsXwsfHB2+++abU7/XXX0diYiLCwsIwevRo3LlzB8nJyThx4gTat28PwDDH6Pvvv49Zs2ahV69e6NWrF1JTU9GzZ09UVFTUav/feOMNbN68GdHR0fD19UV+fj6OHDmCn3/+Gc899xwAYP/+/QgLC4NGo0FcXByMjIykD6zk5GR06NBBts3w8HB4e3tjzpw5SE1NxZo1a+Di4oK5c+cCANavX4/Ro0ejQ4cOGDt2LADAx8fnvnWmpaVh6NChiIqKwvDhw7FgwQL07dsXq1atwrRp0zBu3DgAVe/D8PBwXLx4EUZGVee8P/74I1588UU0a9YMU6dOhZWVFb7++mv0798fW7ZswYABA2RjTZgwAfb29oiLi0NmZibi4+MRHR2NjRs3AgDi4+PrnHUAAPGQ4uLiBADRr18/Wfu4ceMEAHHu3DmRmZkpjI2NxezZs2V9zp8/L0xMTGTtgYGBAoBYtWqVzlh+fn4iMDDwsdZW7ebNmzrrh4SEiJYtW0rL27ZtEwDE999/X+OYycnJAoD48ssvZe3ffvut3nZ9AAi1Wi0yMjKktk8++UQAEG5ubqKoqEhqj42NFQBkfVu0aCEiIiJ0thsYGCh7/jIyMgQAsXbtWqlt/PjxoqbDAICIi4uTlteuXasz9r1jHDhwQAAQTz31lCgvL5falyxZIgCI8+fPCyGEKC8vF46OjuL5558Xt2/flvolJiYKALJt6hv37rEOHDggtel7XefMmSNUKpW4fPmy1FZ9rDyIvjGE0P9cRkRECADigw8+kPV99tlnhUajkZb3798vAIi33npLZzytVnvffanPYzQnJ0eYmZmJ3r17y+qYNm2aAKD3GLuXnZ2dGD9+fI2Pa7Va0bp1axESEqKzr97e3qJHjx5SW/VrNGrUKNk2BgwYIBwdHWVtVlZWeuvTd+y0aNFCABDHjh2T2vbs2SMACAsLC9lxUv0+vPv17969u2jbtq0oKyuT7dcLL7wgWrdurTN2cHCwbF8nTpwojI2NxR9//CG1PWzW3a3O0ynVZwLVJkyYAKDqQtnWrVuh1WoRHh6OvLw86cfNzQ2tW7fGgQMHZOuq1WpERkbWtZSHqq2ahYWF9P+FhYXIy8tDYGAgfv31VxQWFgKA9NvCrl27cPv2bb1jbdq0CXZ2dujRo4dsXzUaDaytrXX2tSbdu3eHl5eXtBwQEAAAGDRoEGxsbHTaf/3111pt11AiIyNl8+WdO3cG8GfdKSkpyM/Px5gxY2Bi8ucvhMOGDYO9vX2dx737dS0tLUVeXh5eeOEFCCH0/lpcH9544w3ZcufOnWWv15YtW6BSqRAXF6ez7t3TWQ19jCYlJaGiogITJkyQ1fHOO+/Uet+bNGmCkydPIjs7W+/jZ8+exaVLlzB06FDk5+dLtZSWlqJ79+44fPgwtFqtbB19z2d+fj6KiopqXde9fH190alTJ2m5+n3VrVs3eHp66rRXv34FBQXYv38/wsPDUVxcLNWfn5+PkJAQXLp0CVevXpWNNXbsWNnz2blzZ1RWVuLy5ct1rv9udZ5Oad26tWzZx8cHRkZGyMzMhJGREYQQOn2q3Tu90KxZs1pfIKusrJTNJwGAg4ODbP371Vbt6NGjiIuLw/Hjx2Xza0DVG8bOzg6BgYEYNGgQZs6cicWLFyMoKAj9+/fH0KFDoVarAQCXLl1CYWEhXFxc9Nabk5MjbfPueVkzMzM4ODhIy3cfOABgZ2cHAPDw8NDbXtv5dkO5d3+qg7m67uoDuFWrVrJ+JiYmsg+zh3XlyhW8//772LFjh85zVB18+hQUFMimDCwsLKTn+mFUz2/fzd7eXlZLeno63N3dZa+/Pg19jFa/Jve+f5ydnWUfrPd7D86bNw8RERHw8PCARqNBr169MGLECLRs2VKqBQAiIiJq3O/CwkLZePc7lmxtbWvczv3U9f2WlpYGIQSmT5+O6dOn6912Tk4OmjVrVqv6H4fHdnfK3Z80Wq0WKpUK33zzjd4rw9bW1rLlu884HiQrKwve3t6ytgMHDtz3QsC9f9SRnp6O7t27o02bNli0aBE8PDxgZmaG3bt3Y/HixdKZgEqlwubNm3HixAns3LkTe/bswahRo7Bw4UKcOHEC1tbW0Gq1cHFxwZdffql37Oo39Ntvv41169ZJ7YGBgbKLZTVdQa+pXdz1r+rV9EcrlZWVBrsyX5u6a+t++3fvco8ePVBQUIApU6agTZs2sLKywtWrVzFy5EidM7y7DRw4EIcOHZKWIyIipAtTtRm72uN6vg1xjNbW/d6D4eHh6Ny5M7Zt24bvvvsO8+fPx9y5c7F161aEhYVJdc+fPx/PPPOM3u3fmw+P81h60DYfNFZ1/TExMQgJCdHb994Tk/qo/251DvFLly7JXsi0tDRotVp4eXnB2NgYQgh4e3vjiSeeqHNx+t5Abm5u2Lt3r6zN39+/1rUBwM6dO1FeXo4dO3bIPiVrmvro2LEjOnbsiNmzZ+Pf//43hg0bhq+++gqjR4+Gj48PkpKS8OKLL973w2jy5MkYPny4tPwoUwb3sre3xx9//KHTfvnyZekMqCaG+qvF6vt209LS0LVrV6n9zp07yMzMRLt27aS26ufq3n2899fR8+fP45dffsG6deswYsQIqf3e40WfhQsXys6M3N3dH2rsh+Hj44M9e/agoKCgxrNxQxyj1a/JpUuXZMdNbm6u7Ll50HuwadOmGDduHMaNG4ecnBw899xzmD17NsLCwqQLjra2tggODq6xlofVUMdx9fNiamraaOqv85x49a1p1ZYtWwYACAsLw8CBA2FsbIyZM2fqfNoIIZCfn1+rMaysrHTePObm5ggODpb93BuI96sN+POT8e7aCgsLsXbtWtl6N27c0Km/+uyhvLwcQNWV88rKSnz44Yc69d+5c0eq39fXV1azRqN50O7Xmo+PD06cOCGbDti1a5fO7Zz6WFlZAdANqfrWvn17ODo6YvXq1bhz547U/uWXX+r8mln9xj98+LDUVllZiU8//VTWT9/rKoTAkiVLHliPRqORvT6+vr4AqoLN2NhYNjZQ9ccZdTVo0CAIIfT+kVV17YY4RoODg2Fqaoply5bJthkfHy9bp6b3YGVlpc6UlYuLC9zd3aVaNBoNfHx8sGDBApSUlOjUc+80TW3py4r64OLigqCgIHzyySf4/fffdR43RP11PhPPyMhAv379EBoaiuPHj2PDhg0YOnSo9Ik8a9YsxMbGSreN2djYICMjA9u2bcPYsWMRExPzwDE0Gg1WrlyJWbNmoVWrVnBxcUG3bt0eubaePXvCzMwMffv2RVRUFEpKSrB69Wq4uLjIXph169ZhxYoVGDBgAHx8fFBcXIzVq1fD1tYWvXr1AlA1LRIVFYU5c+bg7Nmz6NmzJ0xNTXHp0iVs2rQJS5YsweDBg+vyFNfa6NGjsXnzZoSGhiI8PBzp6enYsGHDA2+zAiB9mLz11lsICQmBsbExXn311XqtF6i6JjBjxgxMmDAB3bp1Q3h4ODIzM5GYmAgfHx/ZmYmfnx86duyI2NhY6ez1q6++koU/ALRp0wY+Pj6IiYnB1atXYWtriy1btjzS3KOdnR2GDBmCZcuWQaVSwcfHB7t27ZLmkeuia9eueO2117B06VJcunQJoaGh0Gq1SE5ORteuXREdHW2QY7T6nvbq21t79eqFM2fO4JtvvoGTk9MD96u4uBjNmzfH4MGD4e/vD2trayQlJeH777/HwoULAQBGRkZYs2YNwsLC4Ofnh8jISDRr1gxXr17FgQMHYGtri507dz70c6rRaJCUlIRFixbB3d0d3t7e0kXJxy0hIQEvvfQS2rZtizFjxqBly5a4fv06jh8/jt9++w3nzp176G3WNesA1P0Ww59++kkMHjxY2NjYCHt7exEdHS1u3bol67tlyxbx0ksvCSsrK2FlZSXatGkjxo8fLy5evCj1CQwMFH5+fnrHunbtmujdu7ewsbHRue3sUWvbsWOHaNeunTA3NxdeXl5i7ty54vPPP5fdjpSamir+8Y9/CE9PT6FWq4WLi4vo06ePSElJ0Rn7008/FRqNRlhYWAgbGxvRtm1bMXnyZJGdnf3A5xSAzm1Z1bewzZ8/X9Zefcvbpk2bZO0LFy4UzZo1E2q1Wrz44osiJSWlVrcY3rlzR0yYMEE4OzsLlUolu+0Oj3CL4b316RtbCCGWLl0qWrRoIdRqtejQoYM4evSo0Gg0IjQ0VNYvPT1dBAcHC7VaLVxdXcW0adPE3r17dW7/+umnn0RwcLCwtrYWTk5OYsyYMeLcuXM6Y9f2FkMhhMjNzRWDBg0SlpaWwt7eXkRFRYkffvhB7y2GVlZWOuvrG+vOnTti/vz5ok2bNsLMzEw4OzuLsLAwcfr0aamPIY7RyspKMXPmTNG0aVNhYWEhgoKCxA8//FDjbax3Ky8vF++9957w9/cXNjY2wsrKSvj7+4sVK1bo9D1z5owYOHCgcHR0FGq1WrRo0UKEh4eLffv26Txvubm5snX1HYcXLlwQXbp0ERYWFrLbIWu6xbB37946NT3M+zA9PV2MGDFCuLm5CVNTU9GsWTPRp08fsXnzZp067739U99tqw+bdXdT/f/iiRoFrVYLZ2dnDBw4EKtXrzZ0OUSNXqP+Aiz6aysrK9OZz/3iiy9QUFCgyK+3JTIEnomTwRw8eBATJ07EkCFD4OjoiNTUVHz22Wd46qmncPr0aX65FlEtNMpvMaS/By8vL3h4eGDp0qXSBcsRI0bg448/ZoAT1RLPxImIFIxz4kRECsYQJyJSMM6JK4hWq0V2djZsbGz4j/xSvRFCoLi4GO7u7tJ3aFPjxRBXkOzsbJ1vWSOqL1lZWXX+xzmo4TDEFaT6e8XN/EZCZcy7N2rr8v55hi5BUYqLi9Da21P2PfbUeDHEFaR6CkVlbMYQfwh1/c7pvztO2SkDJ7yIiBSMIU5EpGAMcSIiBWOIExEpGEOciEjBGOJERArGECciUjCGOBGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKRhDnIhIwRjiREQKxhAnIlIwhjgRkYIxxImIFIwhTkSkYAxxIiIFY4gTESkYQ5yISMEY4kRECsYQJyJSMIY4EZGCMcSJiBSMIU5EpGAMcQIAxET2wJH1MchJno/LSR/h64Vj0LqFi6yP2swEi6cOwW/7P0bukQX4z/zX4eJgI+tzK3WZzs+Qns/J+piZmmDG+D64+H8z8ceJRbiwawZGvNyx3vexvh1JPoxB/fuhZYtmsDQzwo7/bq+x74Txb8DSzAjLl8brfby8vBwB7Z+FpZkRzp09Wy/10l+DiaELoMahs6YVVn2djNM/XoaJsTFmRvfFrhXj8eyg2bhZVgEAmPfuQIS95IdhUz5HUcktLJ4yBF8tGI1uoxbLtjUmbgP2HvtJWv6j+Jbs8Q1zI+HqaIM3Zv4b6Vm5aOpsCyOVqv53sp6Vlpaibbt2GDEyEv8IH1Rjv/9u34ZTJ0+iqbt7jX3+GTsZTd3dcf5/5+qjVPoLYYgTAODl6JWy5bFxG5C1fw6e9fXA0dR02FqbY2T/Thg5bR0Off9LVZ8ZX+Lc1n+hQ1svnDqfKa1bWHwL1/OL9Y7T44Wn0FnTCr59Z+JG0U0AwJXfC+pnpxpYSGgYQkLD7tvn6tWreHfiW9ix61sM7N9Hb589336DfXv34t8bN+O7b7+pj1LpL4TTKaSXrY05AOBGYVXQPvuUJ8xMTbD/5EWpzy+Z13Hl9wIEtPOWrRs/dQiy9s1B8hcxOtMkvbu0RepPWZgUEYz0bz/E/7ZNx5x3+sNcbVrPe2R4Wq0WoyNHYOKkGPj6+entc/36dYx/cyzWJH4BS0vLBq6QlIhn4qRDpVJhfswgHDuTjp/SfwcAuDnaoLziNgpL5FMjOfnFcHX8c1585opdOPT9L7hZdhvBHdtgydRwWFuoseKrQwAA7+aOeOGZliiruI1X3l0DxyZWWBIbDocmVoia8WXD7aQBLJw/FyYmJhgX/Zbex4UQGDs6EqPHREGjaY/LmZkNWyApEs/EG1hCQgK8vLxgbm6OgIAAnDp1ytAl6YifOgR+Pk0xIjbxodf9eM0eHD+XgXMXf8PCdUlYtC4JE0d0lx43UqkghEDkP9ch5cfL2HP0J0xZtA3D+3T4S5+Np6aeRsLypfhkzVqoapj/X5mwDCXFxXhvSmwDV0dKxhBvQBs3bsSkSZMQFxeH1NRU+Pv7IyQkBDk5OYYuTbJ4yhD06vw0QsYuw9WcP6T2a/nFUJuZws7aQtbfxdGmxvlvAPj+h8to7mYPM9OqX/qu5RUhO7cQRSVlUp8LGddgZGSEZi5NHuu+NCbHjiQjNycHT/q0gI2FKWwsTHHl8mVMnRyDNq2rpqMOHjiAkyeOo4m1OWwsTPH0U60BAC91eh5jRo00YPXUmDHEG9CiRYswZswYREZGwtfXF6tWrYKlpSU+//xzQ5cGoCrA+3Vth9CoZbicnS977MzPV1Bx+w66dnhCamvdwgWeTR1w8n8ZNW6z3ZPNUFBYiorbdwAAx8/9iqZOdrCyMPtzO54uqKzUyj40/mr+Mew1nDp9Die+PyP9NHV3x8RJMdix61sAwMLFS3Ay5az0+LYd/wcAWP/lV4j7YJYhy6dGjHPiDaSiogKnT59GbOyfvyobGRkhODgYx48fN2BlVeKnhuOVMA2GTFyNkptl0jx3YUkZyspvo6ikDInbj2PuuwNRUHQTxaVlWDR5ME6c+1W6M6VXl6fh4mCDU+czUVZxG90D2mDyqJ6IX79fGmfjNymIHR2KT2cMx4erdsPR3gofvdMf6/57AmXltw2x649NSUkJ0tPSpOXLmRk4d/YsHBwc4OHpCUdHR1l/U1NTuLq54YknnwQAeHh6yh63trYGAHi39EHz5s3ruXpSKoZ4A8nLy0NlZSVcXV1l7a6urrhw4YLedcrLy1FeXi4tFxUV1Vt9UeGdAQB717wtax8TtwEbdp4EAExeuBVaIfCf+a9DbWaCpOMX8PacjVLf23cqERXeGfPeHQiVSoX0rFxMWbQNn289JvUpvVWB3uMSsGjyYBzd8B4KCkuxZe8ZzFixq972raGknk5BaI9u0vKU994FAAx/LQKffrbWUGXRX5xKCCEMXcTfQXZ2Npo1a4Zjx46hU6dOUvvkyZNx6NAhnDx5UmedGTNmYObMmTrt6nZjoTI202kn/QpOLjV0CYpSVFQEN6cmKCwshK2traHLoQfgnHgDcXJygrGxMa5fvy5rv379Otzc3PSuExsbi8LCQuknKyurIUolIgVhiDcQMzMzaDQa7Nu3T2rTarXYt2+f7Mz8bmq1Gra2trIfIqK7cU68AU2aNAkRERFo3749OnTogPj4eJSWliIyMtLQpRGRQjHEG9Arr7yC3NxcvP/++7h27RqeeeYZfPvttzoXO4mIaosXNhWkqKgIdnZ2vLD5kHhh8+HwwqaycE6ciEjBGOJERArGECciUjCGOBGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKRhDnIhIwRjiREQKxhAnIlIwhjgRkYIxxImIFIwhTkSkYAxxIiIFY4gTESkYQ5yISMEY4kRECsYQJyJSMIY4EZGCMcSJiBSMIU5EpGAMcSIiBWOIExEpGEOciEjBGOJERArGECciUjCGOBGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKZiJoQugh3flwHzY2toaugzFsH8+2tAlKIqorDB0CfQQeCZORKRgDHEiIgVjiBMRKRhDnIhIwRjiREQKxhAnIlIwhjgRkYIxxImIFIwhTkSkYAxxIiIFY4gTESkYQ5yISMEY4kRECsYQJyJSMIY4EZGCMcSJiBSMIU5EpGAMcSIiBWOIExEpGEOciEjBGOJERArGECciUjCGOBGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKRhDnIhIwRjipNeR5MMY1L8vvD3dYWGqwo7/bpc9PuuDGfB/ug0c7azQ1NkevUKCcerkSVmfuXNmI6jzC3CwtYSbU5MGq70hxIzqiSMb3kPOkQW4vG8Ovl40Bq1buMj6qM1MsHhqOH47MBe5RxfiPwtGw8XBRmdbw/sG4NTGWNw4sRiX983B4qnhsscH9XgWJ76aivxji3Bx9weYOKJ7ve4bKQtDnPQqLS1F23b+iF+aoPfxVq2fwOIly5Fy5jz2HTyCFi280LdXT+Tm5kp9KioqMHDQEIyJerOhym4wnZ9rhVUbDyNwxAL0eXM5TEyMsWtlNCzNzaQ+82IGoXeXpzFs8mfoOToeTZ3t8NXC0bLtvDW8G2ZG98XCtXvx3ODZ6P3GMiQd/1l6vOeLvlg7eyTWbD4CzZDZePujjZgwvBveeKVLg+0rNW4qIYQwdBFUO0VFRbCzs8P1/ELY2to22LgWpips3LwN/V7uf9/aXB3tsHtPErp2k58prl+XiPfefQfX8v6o30JrYP98dL2P4WRvjaz9HyP49cU4mpoOW2tzZO3/GCOnJWJb0lkAwBNerji3bToCRyzAqfOZaGJjgfQ9szHonVU4eOoXvdtN/GgkTE2MMGzy51Lbm68GYlJEMFqHTa+XfRGVFSg/vxqFhQ17nFHd8EycHllFRQU+W/Mp7Ozs0Ladv6HLMQhba3MAwI3CmwCAZ5/yhJmpCfafuCj1+SXzOq78XoCAdt4AgO4d28DISAV3lyY4s+VfSPv2Q2yYOwrNXZtI66jNTFBWfkc21q3yCjR3s4dnU4d63itSAoY41dnu/9sFpybWaGJtjmVLFmPXN3vh5ORk6LIanEqlwvyYwTh2Jh0/pf8OAHBztEV5xW0UltyS9c3JL4KrY9XZrXdzJxgZqTB5VE+8t2ALhr73GeztLLFrZTRMTYwBAHuP/YyXu/sjqMMTUKlUaOXpgreHV/2m09TZrgH3khorhngDOnz4MPr27Qt3d3eoVCps377d0CU9ksCgrjiZchYHDh9Dz56hGD40HDk5OYYuq8HFx4bDr1VTjJi69qHWU6lUMDM1wbvzNiPp+M84dT4TEbGJaOXpgsDnnwAAfL71KFZ9dRhbl7yBolPxOPTFu9i05zQAQKvVPvZ9IeVhiDeg0tJS+Pv7IyFB/8VCpbGysoJPq1YI6NgRq1Z/BhMTE6xb+5mhy2pQi6cMQa/OTyNkzFJczflDar+WXwS1mSnsrC1k/V0cbXE9v6iqT17Vfy/8ek16PO9GCfL+KIGHm73U9q+l/4XTi+/iyV7vwyt4GlJ+vAwAyLiaX1+7RQpiYugC/k7CwsIQFhZm6DLqjVarRXl5uaHLaDCLpwxBv27+6DlmCS5nywP1zM9XUHH7DroGPInt+84CAFq3cIFnUwec/F8GAOD42V+r2r1cpA8Ae1tLODWxxpXfC2Tb02oFsnMLAQDhoRqcOPcr8m6U1OPekVIwxEmvkpISpKelScuZGRk4d/Ys7B0c4OjoiLlzZqN3n35wa9oU+Xl5+GRlArKvXsXAQUOkda5cuYIbBQXIyrqCyspKnDt7FgDg06oVrK2tG3qXHqv42HC8EtYeQyZ+ipLSMrg6Vt3/XVhShrLy2ygqKUPi9uOY++5AFBSWori0DIumDMGJc7/i1PlMAEDalRzsPHAOC94bjOhZ/0FRSRk+mNAPFzOv41BK1d0qjk2sMCD4WRxOuQRzMxOMeLkjBgY/i56jlxhq16mRYYg3YuXl5bIz26KiogYbO/V0CkKCu0rLU96bBAAY/loElq1YhYsXL2DD+nXIz8uDg6Mj2rd/HkkHkuHr5yet8+GM97Fh/TppuePzzwIA9iQdQJfAoIbZkXoSFV51n/beNe/I2se8vx4bdlb90dPkBVug1Qr8Z8FoqM1MkHTsZ7w9Z6Os/+vT12NezEBsXfomtFqBI6cv4eXxCbhz58/57uF9AzBn4gCoVMDJ/2UgZMwSaUqFiPeJG4hKpcK2bdvQv3//GvvMmDEDM2fO1Glv6PvEla4h7hP/K+F94srCC5uNWGxsLAoLC6WfrKwsQ5dERI0Mp1MaMbVaDbVabegyiKgRY4g3oJKSEqTddbEwIyMDZ8+ehYODAzw9PQ1YGREpFUO8AaWkpKBr1z8vFk6aVHWxMCIiAomJiQaqioiUjCHegIKCgsDryET0OPHCJhGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKRhDnIhIwRjiREQKxhAnIlIwhjgRkYIxxImIFIwhTkSkYAxxIiIFY4gTESkYQ5yISMEY4kRECsYQJyJSMIY4EZGCMcSJiBSMIU5EpGAMcSIiBWOIExEpGEOciEjBGOJERArGECciUjCGOBGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKRhDnIhIwRjiREQKZmLoAqj2hBAAgOKiIgNXoiyissLQJShK9fNVfbxR48YQV5Di4mIAQCtvDwNXQn8HxcXFsLOzM3QZ9AAqwY9bxdBqtcjOzoaNjQ1UKpWhy5EUFRXBw8MDWVlZsLW1NXQ5itCYnzMhBIqLi+Hu7g4jI864NnY8E1cQIyMjNG/e3NBl1MjW1rbRBVJj11ifM56BKwc/ZomIFIwhTkSkYAxxemRqtRpxcXFQq9WGLkUx+JzR48ILm0RECsYzcSIiBWOIExEpGEOciEjBGOJERArGEKdHlpCQAC8vL5ibmyMgIACnTp0ydEmN1uHDh9G3b1+4u7tDpVJh+/bthi6JFI4hTo9k48aNmDRpEuLi4pCamgp/f3+EhIQgJyfH0KU1SqWlpfD390dCQoKhS6G/CN5iSI8kICAAzz//PJYvXw6g6vtdPDw8MGHCBEydOtXA1TVuKpUK27ZtQ//+/Q1dCikYz8SpzioqKnD69GkEBwdLbUZGRggODsbx48cNWBnR3wdDnOosLy8PlZWVcHV1lbW7urri2rVrBqqK6O+FIU5EpGAMcaozJycnGBsb4/r167L269evw83NzUBVEf29MMSpzszMzKDRaLBv3z6pTavVYt++fejUqZMBKyP6++A/CkGPZNKkSYiIiED79u3RoUMHxMfHo7S0FJGRkYYurVEqKSlBWlqatJyRkYGzZ8/CwcEBnp6eBqyMlIq3GNIjW758OebPn49r167hmWeewdKlSxEQEGDoshqlgwcPomvXrjrtERERSExMbPiCSPEY4kRECsY5cSIiBWOIExEpGEOciEjBGOJERArGECciUjCGOBGRgjHEiYgUjCFORKRgDHEiIgVjiBMRKRhDnIhIwRjiREQK9v8AL7wzAt3TsKIAAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"code","source":"# saving the model\ntrainer.save_model('model')","metadata":{"id":"w9e7sc9TqklD","execution":{"iopub.status.busy":"2024-02-18T22:57:49.924762Z","iopub.execute_input":"2024-02-18T22:57:49.925539Z","iopub.status.idle":"2024-02-18T22:57:51.146379Z","shell.execute_reply.started":"2024-02-18T22:57:49.925500Z","shell.execute_reply":"2024-02-18T22:57:51.145550Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhRy3DZdM40O","outputId":"64b21b54-8073-4a45-ddf2-13d5007f4fdf","execution":{"iopub.status.busy":"2024-02-18T22:57:53.274384Z","iopub.execute_input":"2024-02-18T22:57:53.275131Z","iopub.status.idle":"2024-02-18T22:57:53.284141Z","shell.execute_reply.started":"2024-02-18T22:57:53.275099Z","shell.execute_reply":"2024-02-18T22:57:53.283218Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch\n\n# Specify the directory where your fine-tuned model is saved\nmodel_directory = '/kaggle/working/model'\n\n# Load the tokenizer and the model\ntokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')  # Correct tokenizer for DistilBERT\nmodel = BertForSequenceClassification.from_pretrained(model_directory)  # Correct model for DistilBERT\n# Ensure the model is in evaluation mode\nmodel.eval()\n\n# Function to preprocess the text and make predictions\ndef predict_sentiment(text):\n    # Tokenize the text\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n    # Move the tensors to the same device as the model\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Get the model's predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Get the predicted class (assuming binary classification)\n    predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n\n    return predicted_class\n\n# Example usage\ntext = \"مړه یو کس وی صرف ځان شرموی او یو ستا غوندے جاهل وی چې قوم او ملت شرموی\"\npredicted_sentiment = predict_sentiment(text)\nprint(f\"Predicted sentiment for '{text}': {predicted_sentiment}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0OmFhSZqklD","outputId":"768f4846-998c-493c-f078-f5bcd23880fd","execution":{"iopub.status.busy":"2024-02-18T23:04:19.615848Z","iopub.execute_input":"2024-02-18T23:04:19.616262Z","iopub.status.idle":"2024-02-18T23:04:20.394691Z","shell.execute_reply.started":"2024-02-18T23:04:19.616230Z","shell.execute_reply":"2024-02-18T23:04:20.393764Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'مړه یو کس وی صرف ځان شرموی او یو ستا غوندے جاهل وی چې قوم او ملت شرموی': 1\n","output_type":"stream"}]},{"cell_type":"code","source":"text2 = \"زه دا محصول خوښوم!\"\npredicted_sentiment2 = predict_sentiment(text2)\nprint(f\"Predicted sentiment for '{text2}': {predicted_sentiment2}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0_ZqGHIIcw4","outputId":"2b0673e7-4a58-42d7-9aed-faa1231633fe","execution":{"iopub.status.busy":"2024-02-18T23:04:22.574260Z","iopub.execute_input":"2024-02-18T23:04:22.574623Z","iopub.status.idle":"2024-02-18T23:04:22.641785Z","shell.execute_reply.started":"2024-02-18T23:04:22.574592Z","shell.execute_reply":"2024-02-18T23:04:22.640703Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'زه دا محصول خوښوم!': 0\n","output_type":"stream"}]},{"cell_type":"code","source":"text4 = \"هههه حرام تل حرام وي کومه وحې را نازله شوه چې زده کړې حلالې شوې داسې ووایه چې د امریکا سوټي په زور خلاصیږي\"\npredicted_sentiment3 = predict_sentiment(text4)\nprint(f\"Predicted sentiment for '{text4}': {predicted_sentiment3}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T23:04:23.921197Z","iopub.execute_input":"2024-02-18T23:04:23.921579Z","iopub.status.idle":"2024-02-18T23:04:24.007664Z","shell.execute_reply.started":"2024-02-18T23:04:23.921548Z","shell.execute_reply":"2024-02-18T23:04:24.006721Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'هههه حرام تل حرام وي کومه وحې را نازله شوه چې زده کړې حلالې شوې داسې ووایه چې د امریکا سوټي په زور خلاصیږي': 1\n","output_type":"stream"}]},{"cell_type":"code","source":"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8jiuyVkLNTZ","outputId":"7eaa0dce-da0a-4b2f-a8af-61aa0243ab81","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"1bjpFq-ZLqvr","outputId":"c04ec59f-061d-40c9-cd13-58218f3ee946","execution":{"iopub.status.busy":"2024-02-18T17:53:52.722565Z","iopub.execute_input":"2024-02-18T17:53:52.723401Z","iopub.status.idle":"2024-02-18T17:53:53.874866Z","shell.execute_reply.started":"2024-02-18T17:53:52.723365Z","shell.execute_reply":"2024-02-18T17:53:53.873896Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_WoWAfciLxqc","trusted":true},"execution_count":null,"outputs":[]}]}