{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7602802,"sourceType":"datasetVersion","datasetId":4426050},{"sourceId":7639691,"sourceType":"datasetVersion","datasetId":4452458},{"sourceId":7642463,"sourceType":"datasetVersion","datasetId":4454338}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Loading the important libraris","metadata":{"id":"8AqBTNmLGhmN"}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import keras\n# from sklearn.model_selection import train_test_split\n# from keras.preprocessing.text import Tokenizer\n# from keras.utils import pad_sequences\n# from keras.layers import Embedding, Dropout, Dense, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPool1D\n# from keras.models import Sequential\n# import matplotlib.pyplot as plt\n# from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n# import itertools\n# import json","metadata":{"id":"l-YbOJdXGm-a","execution":{"iopub.status.busy":"2024-02-18T21:36:14.485075Z","iopub.execute_input":"2024-02-18T21:36:14.485681Z","iopub.status.idle":"2024-02-18T21:36:28.591427Z","shell.execute_reply.started":"2024-02-18T21:36:14.485644Z","shell.execute_reply":"2024-02-18T21:36:28.590609Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-18 21:36:17.151567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-18 21:36:17.151667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-18 21:36:17.294225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"G41zdyKYGvC3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LjHEIlyLIS_b","outputId":"c932b2e4-7d77-44a8-83ba-523bcfd8a642","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9G25d6saKsx2","outputId":"5c5a4799-8bdc-493c-ebef-8f65d6b2b215","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgLFd_vHqkk9","outputId":"77543024-b101-4a14-d50b-911e90d60266","execution":{"iopub.status.busy":"2024-02-18T21:43:40.755005Z","iopub.execute_input":"2024-02-18T21:43:40.755674Z","iopub.status.idle":"2024-02-18T21:44:03.996584Z","shell.execute_reply.started":"2024-02-18T21:43:40.755645Z","shell.execute_reply":"2024-02-18T21:44:03.995480Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nCollecting transformers\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\nSuccessfully installed transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing important libraries\nimport torch\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"id":"6MKGelDSRbq6","execution":{"iopub.status.busy":"2024-02-19T00:11:30.913422Z","iopub.execute_input":"2024-02-19T00:11:30.913836Z","iopub.status.idle":"2024-02-19T00:11:30.926238Z","shell.execute_reply.started":"2024-02-19T00:11:30.913806Z","shell.execute_reply":"2024-02-19T00:11:30.925207Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# seed everythig\n\nSEED_VAL = 42\ntorch.manual_seed(SEED_VAL)\ntorch.cuda.manual_seed_all(SEED_VAL)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"XXVYHxqCqkk-","execution":{"iopub.status.busy":"2024-02-19T00:11:31.652528Z","iopub.execute_input":"2024-02-19T00:11:31.652899Z","iopub.status.idle":"2024-02-19T00:11:31.658123Z","shell.execute_reply.started":"2024-02-19T00:11:31.652871Z","shell.execute_reply":"2024-02-19T00:11:31.657104Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\n\nMAX_LENGTH = 100\nLR = 2e-5\nBATCH_SIZE = 16\nEPOCHS = 3","metadata":{"id":"LZfAf6RKqkk-","execution":{"iopub.status.busy":"2024-02-19T00:13:39.613245Z","iopub.execute_input":"2024-02-19T00:13:39.613935Z","iopub.status.idle":"2024-02-19T00:13:39.618222Z","shell.execute_reply.started":"2024-02-19T00:13:39.613905Z","shell.execute_reply":"2024-02-19T00:13:39.617269Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# model and tokenizer\n\nMODEL = 'albert-base-v2'","metadata":{"id":"59lnI9X5qkk-","execution":{"iopub.status.busy":"2024-02-19T00:13:40.822450Z","iopub.execute_input":"2024-02-19T00:13:40.823286Z","iopub.status.idle":"2024-02-19T00:13:40.827103Z","shell.execute_reply.started":"2024-02-19T00:13:40.823253Z","shell.execute_reply":"2024-02-19T00:13:40.826232Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# loading the data\ndf = pd.read_csv('/kaggle/input/balanced-data/balanced_data.csv')\ntrain, test = train_test_split(df, test_size=0.2, stratify=df.label, random_state=SEED_VAL)\ntest, val = train_test_split(test, test_size=0.5, stratify=test.label, random_state=SEED_VAL)\n\ntrain_text = list(train.text)\ntrain_labels = list(train.label)\n\nval_text = list(val.text)\nval_labels = list(val.label)\n\ntest_text = list(test.text)\ntest_labels = list(test.label)\n\nprint('Total: ', len(df))\nprint('Train: ', len(train))\nprint('Val: ', len(val))\nprint('Test: ', len(test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oH272kgNqkk-","outputId":"246d449f-1b8f-433d-c2d8-56cd41a7cf95","execution":{"iopub.status.busy":"2024-02-19T00:13:41.862775Z","iopub.execute_input":"2024-02-19T00:13:41.863168Z","iopub.status.idle":"2024-02-19T00:13:42.056216Z","shell.execute_reply.started":"2024-02-19T00:13:41.863135Z","shell.execute_reply":"2024-02-19T00:13:42.055284Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Total:  44000\nTrain:  35200\nVal:  4400\nTest:  4400\n","output_type":"stream"}]},{"cell_type":"code","source":"# df['text'][1421]\ndf","metadata":{"execution":{"iopub.status.busy":"2024-02-19T00:13:42.972432Z","iopub.execute_input":"2024-02-19T00:13:42.973236Z","iopub.status.idle":"2024-02-19T00:13:42.983680Z","shell.execute_reply.started":"2024-02-19T00:13:42.973205Z","shell.execute_reply":"2024-02-19T00:13:42.982687Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0      د جنګ د مفهوم او معنا څخه ناخبره معصوم ماشومان...      0\n1      تردې وروسته يې خپله دوکتورا د وګړپوهنې په څانګ...      0\n2      د افغانستان د کرکټ لوبغاړي اماراتو ته رسېدلي د...      0\n3                     د بابا تازه انځور ته زړونه ورکړئ ❤      0\n4                                       البيت الابراهيمي      0\n...                                                  ...    ...\n43995  ستا مور له عزته سره وغيم فراري دي مور غولي ملي...      1\n43996  ستا مفکوره د پنجاسپیتوب مفکوره ده وړه نجلکۍ یې...      1\n43997  ددناموس وغیم خوچی نیسی یی مردار ه وایی ولی هسی...      1\n43998  ډیر عجب ښه کار وشو داسی خبیث خلک همداسی سزا ور...      1\n43999  د الله لعنت د پرتا وې خاینه فاسده تا د افغانست...      1\n\n[44000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>د جنګ د مفهوم او معنا څخه ناخبره معصوم ماشومان...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>تردې وروسته يې خپله دوکتورا د وګړپوهنې په څانګ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>د افغانستان د کرکټ لوبغاړي اماراتو ته رسېدلي د...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>د بابا تازه انځور ته زړونه ورکړئ ❤</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>البيت الابراهيمي</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43995</th>\n      <td>ستا مور له عزته سره وغيم فراري دي مور غولي ملي...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43996</th>\n      <td>ستا مفکوره د پنجاسپیتوب مفکوره ده وړه نجلکۍ یې...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43997</th>\n      <td>ددناموس وغیم خوچی نیسی یی مردار ه وایی ولی هسی...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43998</th>\n      <td>ډیر عجب ښه کار وشو داسی خبیث خلک همداسی سزا ور...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43999</th>\n      <td>د الله لعنت د پرتا وې خاینه فاسده تا د افغانست...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>44000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenization\n# from transformers import DistilBertTokenizerFast , TFDistilBertForSequenceClassification\n\ntokenizer =AlbertTokenizer.from_pretrained(MODEL)\ntrain_encodings = tokenizer(train_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\nval_encodings = tokenizer(val_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\ntest_encodings = tokenizer(test_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_1_KBJ1qkk_","outputId":"28e483e6-474b-4ce6-f491-ceb5fadaca9e","execution":{"iopub.status.busy":"2024-02-19T00:13:43.902327Z","iopub.execute_input":"2024-02-19T00:13:43.902694Z","iopub.status.idle":"2024-02-19T00:14:01.932789Z","shell.execute_reply.started":"2024-02-19T00:13:43.902665Z","shell.execute_reply":"2024-02-19T00:14:01.931627Z"},"trusted":true},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f4f0a534b1435fb5e923cd0381d9b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8702961f57493a87270846aa911d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818b06d68f294983bf23342101c212cc"}},"metadata":{}}]},{"cell_type":"code","source":"# Torch Dataset\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = MyDataset(train_encodings, train_labels)\nval_dataset = MyDataset(val_encodings, val_labels)\ntest_dataset = MyDataset(test_encodings, test_labels)","metadata":{"id":"tStNX5Coqkk_","execution":{"iopub.status.busy":"2024-02-19T00:14:04.452831Z","iopub.execute_input":"2024-02-19T00:14:04.453200Z","iopub.status.idle":"2024-02-19T00:14:04.460466Z","shell.execute_reply.started":"2024-02-19T00:14:04.453171Z","shell.execute_reply":"2024-02-19T00:14:04.459509Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] -U\n!pip install accelerate -U\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0NprsErswHc","outputId":"686d6d4d-07c0-444e-ff99-7f4827bf737f","execution":{"iopub.status.busy":"2024-02-19T00:14:05.954316Z","iopub.execute_input":"2024-02-19T00:14:05.955062Z","iopub.status.idle":"2024-02-19T00:14:30.955621Z","shell.execute_reply.started":"2024-02-19T00:14:05.955030Z","shell.execute_reply":"2024-02-19T00:14:30.954653Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.37.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.11 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training Arguments\n\n\n!pip install transformers[torch]\n\nfrom transformers import TrainingArguments, Trainer\n\n# Rest of your code\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    logging_steps=50,\n    save_strategy='no',\n    evaluation_strategy='steps',\n    logging_strategy='steps',\n    report_to='none',\n)\n\n\n","metadata":{"id":"y1-C6yBRqkk_","execution":{"iopub.status.busy":"2024-02-19T00:14:30.958215Z","iopub.execute_input":"2024-02-19T00:14:30.958602Z","iopub.status.idle":"2024-02-19T00:14:43.413376Z","shell.execute_reply.started":"2024-02-19T00:14:30.958560Z","shell.execute_reply":"2024-02-19T00:14:43.412400Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.37.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.11 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model and Trainer\n\nmodel = AlbertForSequenceClassification.from_pretrained(MODEL)\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)","metadata":{"id":"tI-Qt-8rqklA","execution":{"iopub.status.busy":"2024-02-19T00:14:43.414913Z","iopub.execute_input":"2024-02-19T00:14:43.415323Z","iopub.status.idle":"2024-02-19T00:14:44.280096Z","shell.execute_reply.started":"2024-02-19T00:14:43.415281Z","shell.execute_reply":"2024-02-19T00:14:44.279075Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3d8460b0c34f18ba57c14533bb4cc6"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Start Fine-tuning\n\ntrainer.train()","metadata":{"id":"15eQckVUqklA","execution":{"iopub.status.busy":"2024-02-19T00:15:14.333557Z","iopub.execute_input":"2024-02-19T00:15:14.334394Z","iopub.status.idle":"2024-02-19T01:08:19.788154Z","shell.execute_reply.started":"2024-02-19T00:15:14.334362Z","shell.execute_reply":"2024-02-19T01:08:19.787225Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6600' max='6600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6600/6600 53:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.719600</td>\n      <td>0.695023</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.711100</td>\n      <td>0.730492</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.711900</td>\n      <td>0.716589</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.703200</td>\n      <td>0.694636</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.709900</td>\n      <td>0.706662</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.703100</td>\n      <td>0.729892</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.706500</td>\n      <td>0.716804</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.711800</td>\n      <td>0.693406</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.698300</td>\n      <td>0.693480</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.702100</td>\n      <td>0.696230</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.703200</td>\n      <td>0.703974</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.705300</td>\n      <td>0.704352</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.699600</td>\n      <td>0.699576</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.701900</td>\n      <td>0.693530</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.696900</td>\n      <td>0.693376</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.695000</td>\n      <td>0.695215</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.693000</td>\n      <td>0.693327</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.697700</td>\n      <td>0.694324</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.690500</td>\n      <td>0.693856</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.698000</td>\n      <td>0.694555</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.698400</td>\n      <td>0.693575</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.695200</td>\n      <td>0.695490</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.696700</td>\n      <td>0.693752</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.695500</td>\n      <td>0.693125</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.694500</td>\n      <td>0.702898</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.700300</td>\n      <td>0.693385</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.694000</td>\n      <td>0.693304</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.693800</td>\n      <td>0.694104</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.695100</td>\n      <td>0.695749</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.695000</td>\n      <td>0.693514</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.693400</td>\n      <td>0.693145</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.693900</td>\n      <td>0.693274</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.697400</td>\n      <td>0.694172</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.694800</td>\n      <td>0.693234</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.692800</td>\n      <td>0.694042</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.698500</td>\n      <td>0.693136</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.692600</td>\n      <td>0.693166</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.692300</td>\n      <td>0.696114</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.695100</td>\n      <td>0.694096</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.694300</td>\n      <td>0.693231</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.694800</td>\n      <td>0.695237</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.694200</td>\n      <td>0.694712</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.696600</td>\n      <td>0.693300</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.694200</td>\n      <td>0.694892</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.693700</td>\n      <td>0.694155</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.693500</td>\n      <td>0.693907</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.693800</td>\n      <td>0.693835</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.696400</td>\n      <td>0.694908</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.693400</td>\n      <td>0.693144</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.695200</td>\n      <td>0.693136</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.695300</td>\n      <td>0.693154</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.690900</td>\n      <td>0.696048</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.694700</td>\n      <td>0.694935</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.695200</td>\n      <td>0.693510</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.696100</td>\n      <td>0.693140</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.693300</td>\n      <td>0.693409</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.693700</td>\n      <td>0.693140</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.696700</td>\n      <td>0.693251</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.694700</td>\n      <td>0.693332</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.694700</td>\n      <td>0.695868</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.698700</td>\n      <td>0.693136</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.692700</td>\n      <td>0.693128</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.694300</td>\n      <td>0.693180</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.692200</td>\n      <td>0.696155</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.694100</td>\n      <td>0.693830</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.694500</td>\n      <td>0.694152</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.693600</td>\n      <td>0.693863</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.694700</td>\n      <td>0.695103</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.691200</td>\n      <td>0.694609</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.697900</td>\n      <td>0.693343</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.691900</td>\n      <td>0.693613</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.692200</td>\n      <td>0.693212</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.694000</td>\n      <td>0.693449</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.693200</td>\n      <td>0.693267</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.693300</td>\n      <td>0.693224</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.693400</td>\n      <td>0.693163</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.693700</td>\n      <td>0.693104</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.693600</td>\n      <td>0.693914</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.692600</td>\n      <td>0.692953</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.695000</td>\n      <td>0.693992</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.692800</td>\n      <td>0.693090</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.693500</td>\n      <td>0.694129</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.694400</td>\n      <td>0.693126</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.692100</td>\n      <td>0.694780</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.698600</td>\n      <td>0.694602</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.694100</td>\n      <td>0.693375</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.694900</td>\n      <td>0.693463</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.691300</td>\n      <td>0.696394</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.694600</td>\n      <td>0.693998</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.696800</td>\n      <td>0.693760</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.693300</td>\n      <td>0.693240</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.691700</td>\n      <td>0.693475</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.696200</td>\n      <td>0.693271</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.695600</td>\n      <td>0.693364</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.693500</td>\n      <td>0.693237</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.695600</td>\n      <td>0.693941</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.696200</td>\n      <td>0.693207</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.693000</td>\n      <td>0.693092</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.694500</td>\n      <td>0.693053</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.694600</td>\n      <td>0.693120</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.696100</td>\n      <td>0.693140</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.694700</td>\n      <td>0.693169</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.693700</td>\n      <td>0.693285</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.693700</td>\n      <td>0.693114</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.692500</td>\n      <td>0.693107</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.694000</td>\n      <td>0.693140</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.695600</td>\n      <td>0.693106</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.692200</td>\n      <td>0.693495</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.694100</td>\n      <td>0.693128</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.692700</td>\n      <td>0.693112</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.693100</td>\n      <td>0.693106</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.694700</td>\n      <td>0.693118</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.695400</td>\n      <td>0.693111</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.694500</td>\n      <td>0.693118</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.695900</td>\n      <td>0.693088</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.694100</td>\n      <td>0.693183</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.692500</td>\n      <td>0.693153</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.692600</td>\n      <td>0.693255</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.692500</td>\n      <td>0.693341</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.695400</td>\n      <td>0.693640</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.693200</td>\n      <td>0.693532</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.695500</td>\n      <td>0.693105</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.694800</td>\n      <td>0.693051</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.695200</td>\n      <td>0.693036</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.690900</td>\n      <td>0.693080</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.693500</td>\n      <td>0.693156</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.692000</td>\n      <td>0.693194</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.693700</td>\n      <td>0.693145</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.694400</td>\n      <td>0.693057</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.694700</td>\n      <td>0.693010</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.694800</td>\n      <td>0.693005</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.693700</td>\n      <td>0.693005</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6600, training_loss=0.6956312867366906, metrics={'train_runtime': 3184.7533, 'train_samples_per_second': 33.158, 'train_steps_per_second': 2.072, 'total_flos': 492898348800000.0, 'train_loss': 0.6956312867366906, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# trainer.evaluate()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"aoWd0sRpwsY_","outputId":"8a023bf5-4411-48c9-d0a0-b3501cbd5e8d","execution":{"iopub.status.busy":"2024-02-18T19:58:18.718980Z","iopub.execute_input":"2024-02-18T19:58:18.719238Z","iopub.status.idle":"2024-02-18T19:58:18.723049Z","shell.execute_reply.started":"2024-02-18T19:58:18.719214Z","shell.execute_reply":"2024-02-18T19:58:18.722221Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# print(logs.columns)\n#","metadata":{"id":"3EJuSreqwQr9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss curves\nlogs = pd.DataFrame(trainer.state.log_history)\nlogs.to_csv('history.txt', sep='\\t', index=None)\n","metadata":{"id":"f64B40YPqklB","execution":{"iopub.status.busy":"2024-02-19T01:09:56.135779Z","iopub.execute_input":"2024-02-19T01:09:56.136505Z","iopub.status.idle":"2024-02-19T01:09:56.150198Z","shell.execute_reply.started":"2024-02-19T01:09:56.136473Z","shell.execute_reply":"2024-02-19T01:09:56.149410Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# # Assuming logs is your DataFrame containing the training logs\n# logs = pd.DataFrame(trainer.state.log_history)\n\n# # Filter out any steps where the loss is not greater than  0\n# loss = logs.loc[logs['train_loss'] >  0]\n\n# # Plot the training loss\n# plt.figure(figsize=(2.8,  2))\n# plt.plot(loss.step, loss['train_loss'], label='Train Loss', lw=1)\n# plt.legend(['Train Loss'], loc='upper right')\n# plt.xlabel('Steps', fontsize=11)\n# plt.ylabel('Loss', fontsize=11)\n# plt.title('Pashto-BERT')\n# plt.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"3wglRrZtxsMC","outputId":"a0c61ba7-1936-41a2-c2b5-187ea1d0c8c3","execution":{"iopub.status.busy":"2024-02-19T01:09:59.462276Z","iopub.execute_input":"2024-02-19T01:09:59.463038Z","iopub.status.idle":"2024-02-19T01:09:59.467127Z","shell.execute_reply.started":"2024-02-19T01:09:59.463005Z","shell.execute_reply":"2024-02-19T01:09:59.466279Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Testing and prediction\npreds_raw, test_labels , _ = trainer.predict(test_dataset)\npreds = np.argmax(preds_raw, axis=-1)\nprint(classification_report(test_labels, preds, digits=4))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"ybG-HZztqklB","outputId":"dd66df7b-12c8-430c-bcad-a903952c3765","execution":{"iopub.status.busy":"2024-02-19T01:10:00.519523Z","iopub.execute_input":"2024-02-19T01:10:00.520241Z","iopub.status.idle":"2024-02-19T01:10:16.557348Z","shell.execute_reply.started":"2024-02-19T01:10:00.520212Z","shell.execute_reply":"2024-02-19T01:10:16.556536Z"},"trusted":true},"execution_count":78,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.5056    0.7141    0.5920      2200\n           1     0.5135    0.3018    0.3802      2200\n\n    accuracy                         0.5080      4400\n   macro avg     0.5096    0.5080    0.4861      4400\nweighted avg     0.5096    0.5080    0.4861      4400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# confusion matrix\ncm = confusion_matrix(test_labels, preds)\nplt.figure(figsize=(2,2))\nplt.imshow(cm, interpolation='nearest', cmap='Blues')\nplt.title('Albert')\ntick_marks = np.arange(2)\nplt.xticks([0,1])\nplt.yticks([0,1])\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"xG6T-L7fqklC","outputId":"26d3ff1f-cd04-40bc-f6e8-efbc9ca4bde9","execution":{"iopub.status.busy":"2024-02-19T01:10:38.190270Z","iopub.execute_input":"2024-02-19T01:10:38.190944Z","iopub.status.idle":"2024-02-19T01:10:38.295959Z","shell.execute_reply.started":"2024-02-19T01:10:38.190911Z","shell.execute_reply":"2024-02-19T01:10:38.295091Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# saving the model\ntrainer.save_model('model')","metadata":{"id":"w9e7sc9TqklD","execution":{"iopub.status.busy":"2024-02-19T01:10:39.681955Z","iopub.execute_input":"2024-02-19T01:10:39.682678Z","iopub.status.idle":"2024-02-19T01:10:39.953905Z","shell.execute_reply.started":"2024-02-19T01:10:39.682647Z","shell.execute_reply":"2024-02-19T01:10:39.952904Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhRy3DZdM40O","outputId":"64b21b54-8073-4a45-ddf2-13d5007f4fdf","execution":{"iopub.status.busy":"2024-02-19T01:10:41.278124Z","iopub.execute_input":"2024-02-19T01:10:41.278476Z","iopub.status.idle":"2024-02-19T01:10:41.285724Z","shell.execute_reply.started":"2024-02-19T01:10:41.278448Z","shell.execute_reply":"2024-02-19T01:10:41.284868Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"AlbertForSequenceClassification(\n  (albert): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): NewGELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (pooler_activation): Tanh()\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\nimport torch\n\n# Specify the directory where your fine-tuned model is saved\nmodel_directory = '/kaggle/working/model'\n\n# Load the tokenizer and the model\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')  # Correct tokenizer for DistilBERT\nmodel = AlbertForSequenceClassification.from_pretrained(model_directory)  # Correct model for DistilBERT\n# Ensure the model is in evaluation model\nmodel.eval()\n\n# Function to preprocess the text and make predictions\ndef predict_sentiment(text):\n    # Tokenize the text\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n    # Move the tensors to the same device as the model\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Get the model's predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Get the predicted class (assuming binary classification)\n    predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n\n    return predicted_class\n\n# Example usage\ntext = \"مړه یو کس وی صرف ځان شرموی او یو ستا غوندے جاهل وی چې قوم او ملت شرموی\"\npredicted_sentiment = predict_sentiment(text)\nprint(f\"Predicted sentiment for '{text}': {predicted_sentiment}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0OmFhSZqklD","outputId":"768f4846-998c-493c-f078-f5bcd23880fd","execution":{"iopub.status.busy":"2024-02-19T01:10:52.343227Z","iopub.execute_input":"2024-02-19T01:10:52.343910Z","iopub.status.idle":"2024-02-19T01:10:52.899382Z","shell.execute_reply.started":"2024-02-19T01:10:52.343876Z","shell.execute_reply":"2024-02-19T01:10:52.898352Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'مړه یو کس وی صرف ځان شرموی او یو ستا غوندے جاهل وی چې قوم او ملت شرموی': 0\n","output_type":"stream"}]},{"cell_type":"code","source":"text2 = \"زه دا محصول خوښوم!\"\npredicted_sentiment2 = predict_sentiment(text2)\nprint(f\"Predicted sentiment for '{text2}': {predicted_sentiment2}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0_ZqGHIIcw4","outputId":"2b0673e7-4a58-42d7-9aed-faa1231633fe","execution":{"iopub.status.busy":"2024-02-19T01:10:54.622337Z","iopub.execute_input":"2024-02-19T01:10:54.622702Z","iopub.status.idle":"2024-02-19T01:10:54.690702Z","shell.execute_reply.started":"2024-02-19T01:10:54.622674Z","shell.execute_reply":"2024-02-19T01:10:54.689764Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'زه دا محصول خوښوم!': 1\n","output_type":"stream"}]},{"cell_type":"code","source":"text4 = \"هههه حرام تل حرام وي کومه وحې را نازله شوه چې زده کړې حلالې شوې داسې ووایه چې د امریکا سوټي په زور خلاصیږي\"\npredicted_sentiment3 = predict_sentiment(text4)\nprint(f\"Predicted sentiment for '{text4}': {predicted_sentiment3}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T01:11:00.672034Z","iopub.execute_input":"2024-02-19T01:11:00.672371Z","iopub.status.idle":"2024-02-19T01:11:00.760297Z","shell.execute_reply.started":"2024-02-19T01:11:00.672346Z","shell.execute_reply":"2024-02-19T01:11:00.759296Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'هههه حرام تل حرام وي کومه وحې را نازله شوه چې زده کړې حلالې شوې داسې ووایه چې د امریکا سوټي په زور خلاصیږي': 0\n","output_type":"stream"}]},{"cell_type":"code","source":"text5 = \"هههه تا ښځه په دا دلیل وغیم بیا خو ګایدن زن ندی حرام زنا حرامه ده کنه\"\npredicted_sentiment4 = predict_sentiment(text5)\nprint(f\"Predicted sentiment for '{text5}': {predicted_sentiment4}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8jiuyVkLNTZ","outputId":"7eaa0dce-da0a-4b2f-a8af-61aa0243ab81","execution":{"iopub.status.busy":"2024-02-19T01:11:04.495247Z","iopub.execute_input":"2024-02-19T01:11:04.496178Z","iopub.status.idle":"2024-02-19T01:11:04.570023Z","shell.execute_reply.started":"2024-02-19T01:11:04.496143Z","shell.execute_reply":"2024-02-19T01:11:04.569246Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'هههه تا ښځه په دا دلیل وغیم بیا خو ګایدن زن ندی حرام زنا حرامه ده کنه': 0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"1bjpFq-ZLqvr","outputId":"c04ec59f-061d-40c9-cd13-58218f3ee946","execution":{"iopub.status.busy":"2024-02-18T17:53:52.722565Z","iopub.execute_input":"2024-02-18T17:53:52.723401Z","iopub.status.idle":"2024-02-18T17:53:53.874866Z","shell.execute_reply.started":"2024-02-18T17:53:52.723365Z","shell.execute_reply":"2024-02-18T17:53:53.873896Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_WoWAfciLxqc","trusted":true},"execution_count":null,"outputs":[]}]}