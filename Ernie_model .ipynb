{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7602802,"sourceType":"datasetVersion","datasetId":4426050},{"sourceId":7639691,"sourceType":"datasetVersion","datasetId":4452458},{"sourceId":7642463,"sourceType":"datasetVersion","datasetId":4454338}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Loading the important libraris","metadata":{"id":"8AqBTNmLGhmN"}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import keras\n# from sklearn.model_selection import train_test_split\n# from keras.preprocessing.text import Tokenizer\n# from keras.utils import pad_sequences\n# from keras.layers import Embedding, Dropout, Dense, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPool1D\n# from keras.models import Sequential\n# import matplotlib.pyplot as plt\n# from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n# import itertools\n# import json","metadata":{"id":"l-YbOJdXGm-a","execution":{"iopub.status.busy":"2024-02-18T21:36:14.485075Z","iopub.execute_input":"2024-02-18T21:36:14.485681Z","iopub.status.idle":"2024-02-18T21:36:28.591427Z","shell.execute_reply.started":"2024-02-18T21:36:14.485644Z","shell.execute_reply":"2024-02-18T21:36:28.590609Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-18 21:36:17.151567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-18 21:36:17.151667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-18 21:36:17.294225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"G41zdyKYGvC3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LjHEIlyLIS_b","outputId":"c932b2e4-7d77-44a8-83ba-523bcfd8a642","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9G25d6saKsx2","outputId":"5c5a4799-8bdc-493c-ebef-8f65d6b2b215","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgLFd_vHqkk9","outputId":"77543024-b101-4a14-d50b-911e90d60266","execution":{"iopub.status.busy":"2024-02-18T21:43:40.755005Z","iopub.execute_input":"2024-02-18T21:43:40.755674Z","iopub.status.idle":"2024-02-18T21:44:03.996584Z","shell.execute_reply.started":"2024-02-18T21:43:40.755645Z","shell.execute_reply":"2024-02-18T21:44:03.995480Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nCollecting transformers\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\nSuccessfully installed transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing important libraries\nimport torch\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, ErnieMForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"id":"6MKGelDSRbq6","execution":{"iopub.status.busy":"2024-02-18T23:08:09.714107Z","iopub.execute_input":"2024-02-18T23:08:09.714520Z","iopub.status.idle":"2024-02-18T23:08:09.724797Z","shell.execute_reply.started":"2024-02-18T23:08:09.714486Z","shell.execute_reply":"2024-02-18T23:08:09.723770Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# seed everythig\n\nSEED_VAL = 42\ntorch.manual_seed(SEED_VAL)\ntorch.cuda.manual_seed_all(SEED_VAL)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"XXVYHxqCqkk-","execution":{"iopub.status.busy":"2024-02-18T23:08:11.290526Z","iopub.execute_input":"2024-02-18T23:08:11.290903Z","iopub.status.idle":"2024-02-18T23:08:11.295981Z","shell.execute_reply.started":"2024-02-18T23:08:11.290866Z","shell.execute_reply":"2024-02-18T23:08:11.295052Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\n\nMAX_LENGTH = 100\nLR = 2e-5\nBATCH_SIZE = 16\nEPOCHS = 3","metadata":{"id":"LZfAf6RKqkk-","execution":{"iopub.status.busy":"2024-02-18T23:08:13.033435Z","iopub.execute_input":"2024-02-18T23:08:13.034356Z","iopub.status.idle":"2024-02-18T23:08:13.038651Z","shell.execute_reply.started":"2024-02-18T23:08:13.034322Z","shell.execute_reply":"2024-02-18T23:08:13.037677Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# model and tokenizer\n\nMODEL = 'susnato/ernie-m-base_pytorch'","metadata":{"id":"59lnI9X5qkk-","execution":{"iopub.status.busy":"2024-02-18T23:08:13.943576Z","iopub.execute_input":"2024-02-18T23:08:13.943997Z","iopub.status.idle":"2024-02-18T23:08:13.948222Z","shell.execute_reply.started":"2024-02-18T23:08:13.943967Z","shell.execute_reply":"2024-02-18T23:08:13.947316Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# loading the data\ndf = pd.read_csv('/kaggle/input/balanced-data/balanced_data.csv')\ntrain, test = train_test_split(df, test_size=0.2, stratify=df.label, random_state=SEED_VAL)\ntest, val = train_test_split(test, test_size=0.5, stratify=test.label, random_state=SEED_VAL)\n\ntrain_text = list(train.text)\ntrain_labels = list(train.label)\n\nval_text = list(val.text)\nval_labels = list(val.label)\n\ntest_text = list(test.text)\ntest_labels = list(test.label)\n\nprint('Total: ', len(df))\nprint('Train: ', len(train))\nprint('Val: ', len(val))\nprint('Test: ', len(test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oH272kgNqkk-","outputId":"246d449f-1b8f-433d-c2d8-56cd41a7cf95","execution":{"iopub.status.busy":"2024-02-18T23:08:15.253521Z","iopub.execute_input":"2024-02-18T23:08:15.254268Z","iopub.status.idle":"2024-02-18T23:08:15.452178Z","shell.execute_reply.started":"2024-02-18T23:08:15.254235Z","shell.execute_reply":"2024-02-18T23:08:15.451149Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Total:  44000\nTrain:  35200\nVal:  4400\nTest:  4400\n","output_type":"stream"}]},{"cell_type":"code","source":"# df['text'][1421]\ndf","metadata":{"execution":{"iopub.status.busy":"2024-02-18T23:08:16.134392Z","iopub.execute_input":"2024-02-18T23:08:16.135238Z","iopub.status.idle":"2024-02-18T23:08:16.146428Z","shell.execute_reply.started":"2024-02-18T23:08:16.135204Z","shell.execute_reply":"2024-02-18T23:08:16.145444Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0      د جنګ د مفهوم او معنا څخه ناخبره معصوم ماشومان...      0\n1      تردې وروسته يې خپله دوکتورا د وګړپوهنې په څانګ...      0\n2      د افغانستان د کرکټ لوبغاړي اماراتو ته رسېدلي د...      0\n3                     د بابا تازه انځور ته زړونه ورکړئ ❤      0\n4                                       البيت الابراهيمي      0\n...                                                  ...    ...\n43995  ستا مور له عزته سره وغيم فراري دي مور غولي ملي...      1\n43996  ستا مفکوره د پنجاسپیتوب مفکوره ده وړه نجلکۍ یې...      1\n43997  ددناموس وغیم خوچی نیسی یی مردار ه وایی ولی هسی...      1\n43998  ډیر عجب ښه کار وشو داسی خبیث خلک همداسی سزا ور...      1\n43999  د الله لعنت د پرتا وې خاینه فاسده تا د افغانست...      1\n\n[44000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>د جنګ د مفهوم او معنا څخه ناخبره معصوم ماشومان...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>تردې وروسته يې خپله دوکتورا د وګړپوهنې په څانګ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>د افغانستان د کرکټ لوبغاړي اماراتو ته رسېدلي د...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>د بابا تازه انځور ته زړونه ورکړئ ❤</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>البيت الابراهيمي</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43995</th>\n      <td>ستا مور له عزته سره وغيم فراري دي مور غولي ملي...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43996</th>\n      <td>ستا مفکوره د پنجاسپیتوب مفکوره ده وړه نجلکۍ یې...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43997</th>\n      <td>ددناموس وغیم خوچی نیسی یی مردار ه وایی ولی هسی...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43998</th>\n      <td>ډیر عجب ښه کار وشو داسی خبیث خلک همداسی سزا ور...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43999</th>\n      <td>د الله لعنت د پرتا وې خاینه فاسده تا د افغانست...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>44000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenization\n# from transformers import DistilBertTokenizerFast , TFDistilBertForSequenceClassification\n\ntokenizer =AutoTokenizer.from_pretrained(MODEL)\ntrain_encodings = tokenizer(train_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\nval_encodings = tokenizer(val_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\ntest_encodings = tokenizer(test_text, truncation=True, padding='max_length', max_length=MAX_LENGTH)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_1_KBJ1qkk_","outputId":"28e483e6-474b-4ce6-f491-ceb5fadaca9e","execution":{"iopub.status.busy":"2024-02-18T23:08:17.194285Z","iopub.execute_input":"2024-02-18T23:08:17.194901Z","iopub.status.idle":"2024-02-18T23:08:32.644677Z","shell.execute_reply.started":"2024-02-18T23:08:17.194868Z","shell.execute_reply":"2024-02-18T23:08:32.643841Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/219 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a84e6eb3bd491bb4b17d3219fb2cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/2.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3069363346114548b1e2ebf96df32fd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531563f802f44000bbd0fde389d52f8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80abbcafeae4d6db9d7d01f335e83ba"}},"metadata":{}}]},{"cell_type":"code","source":"# Torch Dataset\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = MyDataset(train_encodings, train_labels)\nval_dataset = MyDataset(val_encodings, val_labels)\ntest_dataset = MyDataset(test_encodings, test_labels)","metadata":{"id":"tStNX5Coqkk_","execution":{"iopub.status.busy":"2024-02-18T23:08:32.646784Z","iopub.execute_input":"2024-02-18T23:08:32.647086Z","iopub.status.idle":"2024-02-18T23:08:32.653885Z","shell.execute_reply.started":"2024-02-18T23:08:32.647061Z","shell.execute_reply":"2024-02-18T23:08:32.652898Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] -U\n!pip install accelerate -U\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0NprsErswHc","outputId":"686d6d4d-07c0-444e-ff99-7f4827bf737f","execution":{"iopub.status.busy":"2024-02-18T23:08:32.655037Z","iopub.execute_input":"2024-02-18T23:08:32.655318Z","iopub.status.idle":"2024-02-18T23:08:57.831821Z","shell.execute_reply.started":"2024-02-18T23:08:32.655294Z","shell.execute_reply":"2024-02-18T23:08:57.830778Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.37.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.11 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training Arguments\n\n\n!pip install transformers[torch]\n\nfrom transformers import TrainingArguments, Trainer\n\n# Rest of your code\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    logging_steps=50,\n    save_strategy='no',\n    evaluation_strategy='steps',\n    logging_strategy='steps',\n    report_to='none',\n)\n\n# Continue with your Trainer setup and training\n","metadata":{"id":"y1-C6yBRqkk_","execution":{"iopub.status.busy":"2024-02-18T23:08:57.834868Z","iopub.execute_input":"2024-02-18T23:08:57.835339Z","iopub.status.idle":"2024-02-18T23:09:10.145390Z","shell.execute_reply.started":"2024-02-18T23:08:57.835296Z","shell.execute_reply":"2024-02-18T23:09:10.144402Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.37.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.11 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model and Trainer\n\nmodel = ErnieMForSequenceClassification.from_pretrained(MODEL)\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)","metadata":{"id":"tI-Qt-8rqklA","execution":{"iopub.status.busy":"2024-02-18T23:09:10.146869Z","iopub.execute_input":"2024-02-18T23:09:10.147193Z","iopub.status.idle":"2024-02-18T23:10:49.078947Z","shell.execute_reply.started":"2024-02-18T23:09:10.147162Z","shell.execute_reply":"2024-02-18T23:10:49.078066Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00364ae2b41048efa4f90cabd26e7ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01bcaca02fd1404d8856fe5560c95b48"}},"metadata":{}},{"name":"stderr","text":"Some weights of ErnieMForSequenceClassification were not initialized from the model checkpoint at susnato/ernie-m-base_pytorch and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Start Fine-tuning\n\ntrainer.train()","metadata":{"id":"15eQckVUqklA","execution":{"iopub.status.busy":"2024-02-18T23:10:49.080790Z","iopub.execute_input":"2024-02-18T23:10:49.081105Z","iopub.status.idle":"2024-02-19T00:01:40.754959Z","shell.execute_reply.started":"2024-02-18T23:10:49.081078Z","shell.execute_reply":"2024-02-19T00:01:40.754041Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6600' max='6600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6600/6600 50:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.505500</td>\n      <td>0.389726</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.375600</td>\n      <td>0.622378</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.391700</td>\n      <td>0.289345</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.359500</td>\n      <td>0.390335</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.304300</td>\n      <td>0.295250</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.305400</td>\n      <td>0.271883</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.310300</td>\n      <td>0.321693</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.267800</td>\n      <td>0.245512</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.277000</td>\n      <td>0.217135</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.278500</td>\n      <td>0.205228</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.237300</td>\n      <td>0.259205</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.270900</td>\n      <td>0.275312</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.232300</td>\n      <td>0.239286</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.244300</td>\n      <td>0.245382</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.227900</td>\n      <td>0.216822</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.231800</td>\n      <td>0.179558</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.227800</td>\n      <td>0.173550</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.235600</td>\n      <td>0.197970</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.229800</td>\n      <td>0.170901</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.206200</td>\n      <td>0.157558</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.245100</td>\n      <td>0.155266</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.236900</td>\n      <td>0.176428</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.195500</td>\n      <td>0.178319</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.274100</td>\n      <td>0.168278</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.220500</td>\n      <td>0.172651</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.206200</td>\n      <td>0.233807</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.207000</td>\n      <td>0.199655</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.212100</td>\n      <td>0.257754</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.216400</td>\n      <td>0.144426</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.222600</td>\n      <td>0.177395</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.214100</td>\n      <td>0.178796</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.228000</td>\n      <td>0.175503</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.200300</td>\n      <td>0.207200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.201200</td>\n      <td>0.160758</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.168900</td>\n      <td>0.249303</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.226000</td>\n      <td>0.164328</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.156300</td>\n      <td>0.201162</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.201800</td>\n      <td>0.181307</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.195900</td>\n      <td>0.178003</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.219000</td>\n      <td>0.151996</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.202500</td>\n      <td>0.171946</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.225800</td>\n      <td>0.154289</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.175100</td>\n      <td>0.163466</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.203900</td>\n      <td>0.148953</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.142500</td>\n      <td>0.221318</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.187700</td>\n      <td>0.153570</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.159400</td>\n      <td>0.167323</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.094500</td>\n      <td>0.175534</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.169000</td>\n      <td>0.146469</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.129000</td>\n      <td>0.186696</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.131500</td>\n      <td>0.170713</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.142900</td>\n      <td>0.153117</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.119300</td>\n      <td>0.195672</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.137800</td>\n      <td>0.168313</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.129500</td>\n      <td>0.176910</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.165900</td>\n      <td>0.172501</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.161600</td>\n      <td>0.157364</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.145900</td>\n      <td>0.163952</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.156200</td>\n      <td>0.155426</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.131500</td>\n      <td>0.161782</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.139800</td>\n      <td>0.174649</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.159200</td>\n      <td>0.167959</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.098800</td>\n      <td>0.226276</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.142500</td>\n      <td>0.182184</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.146800</td>\n      <td>0.162005</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.115000</td>\n      <td>0.169563</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.122600</td>\n      <td>0.202804</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.112700</td>\n      <td>0.181505</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.152600</td>\n      <td>0.167778</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.168900</td>\n      <td>0.162693</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.140300</td>\n      <td>0.176741</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.178900</td>\n      <td>0.151222</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.112800</td>\n      <td>0.171638</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.121800</td>\n      <td>0.183396</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.138500</td>\n      <td>0.169324</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.132400</td>\n      <td>0.167664</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.138300</td>\n      <td>0.196325</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.130800</td>\n      <td>0.183086</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.141300</td>\n      <td>0.173311</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.150900</td>\n      <td>0.144010</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.118000</td>\n      <td>0.158664</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.085300</td>\n      <td>0.181880</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.162500</td>\n      <td>0.156463</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.149700</td>\n      <td>0.137998</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.139800</td>\n      <td>0.145983</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.152200</td>\n      <td>0.127446</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.107800</td>\n      <td>0.155115</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.151400</td>\n      <td>0.152422</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.092200</td>\n      <td>0.167782</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.084800</td>\n      <td>0.175500</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.101400</td>\n      <td>0.164151</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.115900</td>\n      <td>0.164238</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.060600</td>\n      <td>0.173874</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.104500</td>\n      <td>0.186039</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.126800</td>\n      <td>0.166312</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.078600</td>\n      <td>0.173500</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.051600</td>\n      <td>0.198574</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.097700</td>\n      <td>0.176309</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.073800</td>\n      <td>0.177945</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.078600</td>\n      <td>0.175010</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.073500</td>\n      <td>0.194220</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.114700</td>\n      <td>0.173041</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.073100</td>\n      <td>0.182983</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.124400</td>\n      <td>0.181792</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.054800</td>\n      <td>0.182631</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.102400</td>\n      <td>0.169022</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.114000</td>\n      <td>0.164670</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.099500</td>\n      <td>0.165621</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.080900</td>\n      <td>0.177258</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.077300</td>\n      <td>0.178365</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.076400</td>\n      <td>0.191072</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.085700</td>\n      <td>0.187348</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.076800</td>\n      <td>0.189228</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.096700</td>\n      <td>0.184532</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.071400</td>\n      <td>0.185547</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.087300</td>\n      <td>0.172410</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.069300</td>\n      <td>0.186579</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.113900</td>\n      <td>0.179035</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.086700</td>\n      <td>0.184485</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.087700</td>\n      <td>0.187898</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.090700</td>\n      <td>0.172568</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.048900</td>\n      <td>0.169567</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.081700</td>\n      <td>0.181693</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.061500</td>\n      <td>0.177675</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.111400</td>\n      <td>0.168526</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.082300</td>\n      <td>0.166656</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.082000</td>\n      <td>0.165919</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.115000</td>\n      <td>0.166674</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.082800</td>\n      <td>0.168855</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.089300</td>\n      <td>0.170491</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.086200</td>\n      <td>0.172726</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.079400</td>\n      <td>0.172378</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6600, training_loss=0.1570777441516067, metrics={'train_runtime': 3051.0585, 'train_samples_per_second': 34.611, 'train_steps_per_second': 2.163, 'total_flos': 5426665516800000.0, 'train_loss': 0.1570777441516067, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# trainer.evaluate()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"aoWd0sRpwsY_","outputId":"8a023bf5-4411-48c9-d0a0-b3501cbd5e8d","execution":{"iopub.status.busy":"2024-02-18T19:58:18.718980Z","iopub.execute_input":"2024-02-18T19:58:18.719238Z","iopub.status.idle":"2024-02-18T19:58:18.723049Z","shell.execute_reply.started":"2024-02-18T19:58:18.719214Z","shell.execute_reply":"2024-02-18T19:58:18.722221Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# print(logs.columns)\n#","metadata":{"id":"3EJuSreqwQr9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss curves\nlogs = pd.DataFrame(trainer.state.log_history)\nlogs.to_csv('history.txt', sep='\\t', index=None)\n","metadata":{"id":"f64B40YPqklB","execution":{"iopub.status.busy":"2024-02-18T21:17:07.174304Z","iopub.execute_input":"2024-02-18T21:17:07.175061Z","iopub.status.idle":"2024-02-18T21:17:07.189063Z","shell.execute_reply.started":"2024-02-18T21:17:07.175031Z","shell.execute_reply":"2024-02-18T21:17:07.188115Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# # Assuming logs is your DataFrame containing the training logs\n# logs = pd.DataFrame(trainer.state.log_history)\n\n# # Filter out any steps where the loss is not greater than  0\n# loss = logs.loc[logs['train_loss'] >  0]\n\n# # Plot the training loss\n# plt.figure(figsize=(2.8,  2))\n# plt.plot(loss.step, loss['train_loss'], label='Train Loss', lw=1)\n# plt.legend(['Train Loss'], loc='upper right')\n# plt.xlabel('Steps', fontsize=11)\n# plt.ylabel('Loss', fontsize=11)\n# plt.title('Pashto-BERT')\n# plt.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"3wglRrZtxsMC","outputId":"a0c61ba7-1936-41a2-c2b5-187ea1d0c8c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing and prediction\npreds_raw, test_labels , _ = trainer.predict(test_dataset)\npreds = np.argmax(preds_raw, axis=-1)\nprint(classification_report(test_labels, preds, digits=4))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"ybG-HZztqklB","outputId":"dd66df7b-12c8-430c-bcad-a903952c3765","execution":{"iopub.status.busy":"2024-02-19T00:01:56.607268Z","iopub.execute_input":"2024-02-19T00:01:56.607935Z","iopub.status.idle":"2024-02-19T00:02:10.306900Z","shell.execute_reply.started":"2024-02-19T00:01:56.607893Z","shell.execute_reply":"2024-02-19T00:02:10.305877Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9700    0.9541    0.9620      2200\n           1     0.9548    0.9705    0.9626      2200\n\n    accuracy                         0.9623      4400\n   macro avg     0.9624    0.9623    0.9623      4400\nweighted avg     0.9624    0.9623    0.9623      4400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# confusion matrix\ncm = confusion_matrix(test_labels, preds)\nplt.figure(figsize=(2,2))\nplt.imshow(cm, interpolation='nearest', cmap='Blues')\nplt.title('ernie-m-base_pytorch')\ntick_marks = np.arange(2)\nplt.xticks([0,1])\nplt.yticks([0,1])\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"xG6T-L7fqklC","outputId":"26d3ff1f-cd04-40bc-f6e8-efbc9ca4bde9","execution":{"iopub.status.busy":"2024-02-19T00:02:20.904529Z","iopub.execute_input":"2024-02-19T00:02:20.905188Z","iopub.status.idle":"2024-02-19T00:02:21.012297Z","shell.execute_reply.started":"2024-02-19T00:02:20.905155Z","shell.execute_reply":"2024-02-19T00:02:21.011388Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# saving the model\ntrainer.save_model('model')","metadata":{"id":"w9e7sc9TqklD","execution":{"iopub.status.busy":"2024-02-19T00:02:25.395199Z","iopub.execute_input":"2024-02-19T00:02:25.395570Z","iopub.status.idle":"2024-02-19T00:02:28.548092Z","shell.execute_reply.started":"2024-02-19T00:02:25.395540Z","shell.execute_reply":"2024-02-19T00:02:28.547301Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhRy3DZdM40O","outputId":"64b21b54-8073-4a45-ddf2-13d5007f4fdf","execution":{"iopub.status.busy":"2024-02-19T00:02:33.810341Z","iopub.execute_input":"2024-02-19T00:02:33.811141Z","iopub.status.idle":"2024-02-19T00:02:33.820065Z","shell.execute_reply.started":"2024-02-19T00:02:33.811108Z","shell.execute_reply":"2024-02-19T00:02:33.818965Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"ErnieMForSequenceClassification(\n  (ernie_m): ErnieMModel(\n    (embeddings): ErnieMEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ErnieMEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x ErnieMEncoderLayer(\n          (self_attn): ErnieMAttention(\n            (self_attn): ErnieMSelfAttention(\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (pooler): ErnieMPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\nimport torch\n\n# Specify the directory where your fine-tuned model is saved\nmodel_directory = '/kaggle/working/model'\n\n# Load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained('susnato/ernie-m-base_pytorch')  # Correct tokenizer for DistilBERT\nmodel = ErnieMForSequenceClassification.from_pretrained(model_directory)  # Correct model for DistilBERT\n# Ensure the model is in evaluation model\nmodel.eval()\n\n# Function to preprocess the text and make predictions\ndef predict_sentiment(text):\n    # Tokenize the text\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n    # Move the tensors to the same device as the model\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Get the model's predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Get the predicted class (assuming binary classification)\n    predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n\n    return predicted_class\n\n# Example usage\ntext = \"مړه یو کس وی صرف ځان شرموی او یو ستا غوندے جاهل وی چې قوم او ملت شرموی\"\npredicted_sentiment = predict_sentiment(text)\nprint(f\"Predicted sentiment for '{text}': {predicted_sentiment}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0OmFhSZqklD","outputId":"768f4846-998c-493c-f078-f5bcd23880fd","execution":{"iopub.status.busy":"2024-02-19T00:04:49.194505Z","iopub.execute_input":"2024-02-19T00:04:49.194901Z","iopub.status.idle":"2024-02-19T00:04:50.786185Z","shell.execute_reply.started":"2024-02-19T00:04:49.194864Z","shell.execute_reply":"2024-02-19T00:04:50.785240Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'مړه یو کس وی صرف ځان شرموی او یو ستا غوندے جاهل وی چې قوم او ملت شرموی': 1\n","output_type":"stream"}]},{"cell_type":"code","source":"text2 = \"زه دا محصول خوښوم!\"\npredicted_sentiment2 = predict_sentiment(text2)\nprint(f\"Predicted sentiment for '{text2}': {predicted_sentiment2}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0_ZqGHIIcw4","outputId":"2b0673e7-4a58-42d7-9aed-faa1231633fe","execution":{"iopub.status.busy":"2024-02-19T00:04:53.066323Z","iopub.execute_input":"2024-02-19T00:04:53.066693Z","iopub.status.idle":"2024-02-19T00:04:53.132770Z","shell.execute_reply.started":"2024-02-19T00:04:53.066661Z","shell.execute_reply":"2024-02-19T00:04:53.131854Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'زه دا محصول خوښوم!': 0\n","output_type":"stream"}]},{"cell_type":"code","source":"text4 = \"هههه حرام تل حرام وي کومه وحې را نازله شوه چې زده کړې حلالې شوې داسې ووایه چې د امریکا سوټي په زور خلاصیږي\"\npredicted_sentiment3 = predict_sentiment(text4)\nprint(f\"Predicted sentiment for '{text4}': {predicted_sentiment3}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T00:04:54.374528Z","iopub.execute_input":"2024-02-19T00:04:54.375220Z","iopub.status.idle":"2024-02-19T00:04:54.457612Z","shell.execute_reply.started":"2024-02-19T00:04:54.375188Z","shell.execute_reply":"2024-02-19T00:04:54.456588Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'هههه حرام تل حرام وي کومه وحې را نازله شوه چې زده کړې حلالې شوې داسې ووایه چې د امریکا سوټي په زور خلاصیږي': 1\n","output_type":"stream"}]},{"cell_type":"code","source":"text5 = \"هههه تا ښځه په دا دلیل وغیم بیا خو ګایدن زن ندی حرام زنا حرامه ده کنه\"\npredicted_sentiment4 = predict_sentiment(text5)\nprint(f\"Predicted sentiment for '{text5}': {predicted_sentiment4}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8jiuyVkLNTZ","outputId":"7eaa0dce-da0a-4b2f-a8af-61aa0243ab81","execution":{"iopub.status.busy":"2024-02-19T00:09:24.905662Z","iopub.execute_input":"2024-02-19T00:09:24.906338Z","iopub.status.idle":"2024-02-19T00:09:24.979327Z","shell.execute_reply.started":"2024-02-19T00:09:24.906307Z","shell.execute_reply":"2024-02-19T00:09:24.978401Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Predicted sentiment for 'هههه تا ښځه په دا دلیل وغیم بیا خو ګایدن زن ندی حرام زنا حرامه ده کنه': 1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"1bjpFq-ZLqvr","outputId":"c04ec59f-061d-40c9-cd13-58218f3ee946","execution":{"iopub.status.busy":"2024-02-18T17:53:52.722565Z","iopub.execute_input":"2024-02-18T17:53:52.723401Z","iopub.status.idle":"2024-02-18T17:53:53.874866Z","shell.execute_reply.started":"2024-02-18T17:53:52.723365Z","shell.execute_reply":"2024-02-18T17:53:53.873896Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_WoWAfciLxqc","trusted":true},"execution_count":null,"outputs":[]}]}